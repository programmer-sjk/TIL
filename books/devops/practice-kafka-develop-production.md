# 실전 카프카 개발부터 운영까지

- [링크](https://www.yes24.com/Product/Goods/104410708)

## 1. 카프카 개요

- 아파치 카프카와 컨플루언트 카프카를 비교하자면 아파치 카프카를 자동차의 핵심 엔진이라고 비유할 때,
  컨플루언트 카프카는 내비게이션, 각종 편의기능이 모두 탑재된 완성된 자동차라고 할 수 있다.

### 카프카의 주요 특징

- 높은 처리량과 낮은 지연시간
- 높은 확장성
- 고 가용성 / 내구성
- 개발 / 운영 / 관리 편의성

### 카프카의 성장

- 2011년 카프카가 공개된 후 추가된 기능들을 살펴보자
  - v0.8: 리플리케이션 추가
    - 브로커에 장애가 발생해도 복제 기능으로 데이터 유실 없이 안정적으로 사용이 가능해짐
  - v0.8.2: 스키마 레지스트리
    - 프로듀서 / 컨슈머 간에 서로 데이터 구조를 설명할 수 있는 스키마를 등록할 수 있는 기능
  - v0.9: 카프카 커넥트
    - 다양한 시스템과 프로토콜을 별도의 코드 작성 없이도 연동이 가능해짐
  - v0.10: 카프카 스트림즈
    - 실시간 처리를 간편하게 제공
  - v3.0: 주키퍼 의존성 해방
    - 분산 코디네이터 시스템은 주키퍼는 카프카가 높은 성능을 갖는데 장벽이 되어옴
    - 주키퍼 의존성이 제거된 정식 카프카를 발표했지만 아직 운영 환경에서 사용은 추천하지 않음
    - 프로듀서의 전송 보장에 대해 중복 없는 전송(멱등성) 방식을 기본값으로 채택

## 3. 카프카 기본 개념과 구조

### 3.1 카프카 기초 다지기

- 카프카를 구성하는 주요 요소
  - 프로듀서 / 컨슈머 / 토픽 / 파티션 / 메시지
  - 주키퍼: 카프카의 메타데이터 관리 및 정상 상태 점검(health check)을 담당
  - 카프카 클러스터: 여러 대의 브로커로 구성된 클러스터를 의미
  - 브로커: 카프카 어플리케이션이 설치된 서버
  - 세그먼트: 메시지가 브로커의 로컬 디스크에 저장되는 파일을 의미

#### 3.1.1 리플리케이션

- 각 메시지를 복제해 카프카 클러스터 내 브로커들에 분산시키는 동작을 의미한다. 리플리케이션 덕분에 하나의 브로커가 종료되더라도 카프카는 안정성을 유지할 수 있다.
- replication-factor 옵션으로 지정할 수 있고 factor 수가 커지면 안정성은 높아지지만 브로커 리소스를 많이 사용하게 된다. 따라서 아래와 같은 기준을 세우는게 좋다.
  - 테스트나 개발환경: 팩터 수 1
  - 운영 환경(로그성 메시지나 유실 허용): 팩터 수 2
  - 운영 환경(유실 허용하지 않음): 팩터 수 3
- 저자의 경험 상 팩터 수가 3일 경우 메시지 안정성도 보장하고 적절한 디스크 공간을 사용가능

#### 3.1.2 파티션

- 토픽 하나의 처리량을 늘리기 위해 토픽 내에 여러 파티션을 둘 수 있다. 파티션 수는 줄일 수 없기 때문에 초기에 토픽을 생성할 땐 파티션 수를 작게 2개나 4개 정도로 생성 후 메시지 처리량이나 컨슈머 LAG을 모니터링 하면서 조금씩 늘려가는게 좋다.

#### 3.1.3 세그먼트

- 프로듀서가 전송한 메시지는 토픽의 파티션에 저장되며, 각 메시지들은 세그먼트라는 로그 파일의 형태로 브로커의 로컬 디스크에 저장된다.

### 3.2 카프카 핵심 개념

- 카프카가 높은 처리량과 안정성을 지니게 된 특성들을 정리한다.

#### 3.2.1 분산 시스템

- 분산 시스템은 단일 시스템이 갖지 못한 높은 성능을 목표로 하며, 하나의 서버에 장애가 발생할 때 다른 서버가 대신 처리하므로 장애 대응이 탁월하고 부하가 높은 경우 시스템 확장이 용이하다는 장점이 있다.
- 카프카도 분산 시스템이므로 높은 메시지 처리량이 필요할 경우 브로커를 추가하는 방식으로 확장이 가능하고 이는 카프카의 큰 장점 중 하나이다.

#### 3.2.2 페이지 캐시

- OS는 성능을 높이기 위해 꾸준히 개선되고 있는데 페이지 캐시 활용이 대표적이다. 카프카는 OS의 페이지 캐시를 활용하는 방식으로 설계되어 디스크 I/O를 줄여 성능을 높일 수 있다.

#### 3.2.3 배치전송 처리

- 카프카는 프로듀서 / 컨슈머와 통신하며 수 많은 메시지를 주고받는다. 수 많은 통신을 묶어서 처리한다면 단건으로 통신할 때에 비해 네트워크 오버헤드를 줄일 수 있고 장기적으로 더욱 빠르고 효율적으로 처리할 수 있다.

#### 3.2.4 압축 전송

- 카프카는 메시지 전송 시 성능이 높은 압축 전송을 사용하는 것을 권장한다. 높은 압축률이 필요하면 gzip, zstd를 권장하고 빠른 응답 속도가 필요하다면 lz4나 snappy를 권장한다.

#### 3.2.5 토픽, 파티션, 오프셋

- 카프카는 토픽에 데이터를 저장하고 토픽은 병렬 처리를 위해 여러 개의 파티션으로 나뉜다. 파티션의 메시지가 저장되는 위치를 오프셋이라고 부르며 오프셋은 순차적으로 증가하는 숫자 형태로 되어 있다. 각 파티션마다 오프셋은 고유한 숫자로 카프카는 오프셋을 통해 메시지의 순서를 보장하고 컨슈머가 마지막까지 읽은 위치를 알 수 있다.

#### 3.2.6 고가용성 보장

- 앞에 말한 것처럼 카프카는 분산 시스템이기 때문에 하나의 서버가 다운되어도 안정적인 서비스가 가능하다.
- 고가용성 보장을 위해 카프카는 리플리케이션 기능을 제공한다. 복제 기능은 토픽의 파티션을 복제하는 것으로 리더 파티션과 팔로워 파티션이라는 용어를 사용한다.
- 팔로워 수만큼 브로커의 디스크 공간도 소비되므로 일반적으로 리플리케이션 팩터 수를 3으로 구성하도록 권장한다. 리더는 프로듀서 / 컨슈머로부터 오는 모든 읽기 / 쓰기 요청을 처리하고, 팔로워는 오직 리더로부터 데이터를 복제하게 된다.

#### 3.2.7 주키퍼의 의존성

- 주키퍼는 하둡의 서브 프로젝트 중 하나로 출발해 2011년 아파치 탑 레벨 프로젝트로 승격되었다. 오늘날 아파치 산하 프로젝트인 카프카, 하둡, HBase 등 많은 분산 어플리케이션의 코디네이터 역할을 하는 어플리케이션으로 사용되고 있다.
- 주키퍼는 여러 대의 서버로 구성되고, 살아 있는 노드수가 과반수 이상이 유지되면 지속적인 서비스가 가능한 구조이다. 따라서 주키퍼는 반드시 홀수로 구성되어야 한다.
- 주키퍼는 브로커의 노드, 토픽, 컨트롤러를 관리하는 중요한 역할을 맡지만 카프카가 성장하면서 주키퍼 성능의 한계가 드러나기 시작했다. 현재는 카프카에서 주키퍼의 의존성을 제거하려는 움직임이 진행중이며 실제로 현재 주키퍼가 삭제된 카프카 버전이 릴리즈 되었다.

### 3.3 프로듀서의 기본동작과 예제 맛보기

- 프로듀서가 카프카로 레코드를 전송할 때 특정 토픽으로 메시지를 전송한다. 따라서 레코드에서 토픽과 메시지는 필수값이며 특정 파티션에 저장하기 위한 파티션이나 키는 선택사항(옵셔널 값)이다.
- 만약 프로듀서가 레코드의 파티션을 지정했다면 파티셔는 아무 동작을 하지 않고 지정된 파티션으로 레코드를 전달한다. 만약 파티션을 지정하지 않으면 키를 가지고 파티션을 선택해 레코드를 전달하는데 키가 없다면 라운드 로빈(Round Robin) 방식으로 동작한다.
- 프로듀서 내부의 send() 메소드 동작이후 배치 전송을 위해 레코드들을 파티션별로 잠시 모아두게 된다. 전송이 실패하면 재시도 동작이 이뤄지고, 지정된 횟수만큼 재시도가 실패하면 최종 실패하게 된다. 전송이 성공하면 메타데이터를 리턴하게 된다.
- 프로듀서 주요 옵션
  - acks
    - 0: 빠른 전송을 의미하지만 메시지 손실 가능
    - 1: 리더가 메시지를 받았는지 확인하지만 팔로워는 확인하지 않음
    - all: 팔로워가 데이터를 복제했는지 확인. 다소 느리지만 메시지가 손실되지 않음
  - enable.idempotence
    - 활성화되면 중복 없는 전송이 가능하며 동시에 max.in.flight.requests.per.conenction은 5 이하, retries는 0 이상, acks는 all로 설정해야 한다.
  - max.in.flight.requests.per.conenction
    - 하나의 커넥션에서 프로듀서가 최대한 ACK 없이 전송할 수 있는 요청수. 메시지 순서가 중요하다면 1로 설정하는 것을 권장하지만 성능은 다소 떨어진다.
  - retires: 전송에 실패한 데이터를 다시 보내는 횟수
  - transactional.id: 정확히 한 번 전송을 위해 사용하며 enable.idempotence을 true로 설정해야 한다.

### 3.4 컨슈머의 기본 동작과 예제 맛보기

- 컨슈머 그룹은 하나 이상의 컨슈머들이 모여 있는 그룹이고, 컨슈머는 반드시 컨슈머 그룹에 속하게 된다. 컨슈머 그룹은 각 파티션의 리더에게 카프카 토픽에 저장된 메시지를 가져오기 위한 요청을 보낸다. 이때 파티션과 컨슈머 수는 일대일로 매핑되는게 이상적이다. 만약 컨슈머 수가 파티션 수보다 많으면 그냥 대기상태로 존재하게 된다.
- 컨슈머 주요 옵션
  - group.id: 컨슈머가 속한 컨슈머 그룹의 식별자
  - enable.auto.commit: 백그라운드로 주기적으로 오프셋을 커밋
  - isolation.level
    - 트랜잭션 컨슈머에서 사용되는 옵션으로 read_uncommitted는 기본값으로 모든 메시지를 읽는다.
    - read_committed는 트랜잭션이 완료된 메시지만 읽는다.
  - max.poll.records: 한 번의 poll() 요청으로 가져오는 최대 메시지 개수
- 컨슈머에서 메시지를 가져오는 방법은 크게 오토 커밋, 동기 가져오기, 비동기 가져오기가 있다.
  - 컨슈머 어플리케이션은 기본적으로 오토 커밋을 사용한다. 오프셋을 주기적으로 커밋하므로 관리자가 따로 관리하지 않아도 되는 장점이 있는 반면, 컨슈머 종료가 빈번히 일어나면 일부 메시지를 못 가져오거나 중복으로 가져오는 경우가 있다. 하지만 카프카는 굉장히 안정적으로 잘 동작하고, 자주 변경되거나 종료되는 현상이 없어 오토 커밋을 사용하는 경우가 많다.
  - 동기 방식으로 가져오면 속도는 느리지만 메시지 손실은 거의 발생하지 않는다. 여기서 메시지 손실이란 토픽에는 메시지가 존재하지만 잘못된 오프셋 커밋으로 컨슈머가 메시지를 가져오지 못하는 경우를 말한다. 메시지가 손실되면 안 되는 중요한 처리 작업들은 동기 방식으로 진행하길 권장한다. 다만 이 방법도 메시지 중복 이슈는 피할 수 없다.
  - 비 동기 방식은 동기 방식과 달리 오프셋 커밋이 실패해도 재시도 하지 않는다. 예를 들어 1~5 오프셋이 들어왔는데 2가 실패하고 1,3,4 5번이 성공했다고 가정하자. 만약 재시도를 해서 2번이 성공해서 오프셋을 2로 덮어 씌우면 3,4,5를 중복해서
    메시지를 가져오게 된다.
- 컨슈머는 컨슈머 그룹 안에 속하는 것이 일반적으로, 토픽의 파티션과 일대일로 매핑되어 메시지를 가져오게 된다.

## 4. 카프카의 내부 동작원리와 구현

### 4.1 카프카 리플리케이션

- 카프카는 많은 데이터 파이프라인의 중앙에서 메인 허브 역할을 한다. 이런 카프카 클러스터가 정상적으로 동작하지 못할 경우 심각한 문제가 발생할 수 있다. 따라서 카프카는 초기 설계 단계부터 브로커 한 두대에서 장애가 발생해도 안정적으로 운영되도록 설계되었다.

#### 4.1.1 리플리케이션 동작 개요

- 카프카는 리플리케이션 팩터라는 옵션으로 지정한 수만큼 복제본을 가질 수 있다. 만약 총 3개의 리플리케이션이 있으면 그 중 2대의 브로커에 장애가 발생해도 남은 1대의 브로커가 요청을 안전하게 처리할 수 있다.

#### 4.1.2 리더와 팔로워

- 리더는 리플리케이션 중 하나가 선정되며, 모든 읽기 / 쓰기는 리더를 통해서만 가능하다. 즉 프로듀서는 모든 리플리케이션에게 메시지를 보내는게 아니라 리더에게만 메시지를 전송한다. 또한 컨슈머도 오직 리더로부터 메시지를 가져온다.
- 팔로워들은 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 리더로부터 복제한다.

#### 4.1.3 복제 유지와 커밋

- 리더와 팔로워는 ISR(InSyncReplica) 그룹으로 묶여있는데, 이 ISR 그룹안에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있다. 반대로 ISR에 속하지 못한 팔로워는 새로운 리더가 될 수 없다.
- ISR 내의 팔로워들은 지속적으로 리더의 데이터를 복제해가는데, 팔로워가 네트워크 오류나 브로커 장애로 복제를 못하는 경우가 생길수도 있다. 이렇게 뒤처진 팔로워는 리더와의 데이터가 불일치 상태가 되므로, 이 팔로워에게 새로운 리더를 넘겨준다면 데이터의 정합성이나 메시지 손실 등의 문제가 발생할 수 있다.
- 따라서 리더는 팔로워들이 뒤쳐지지 않고 복제를 잘 하고 있는지를 감시한다. 만약 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않으면 리더는 팔로워에 문제가 발생했다고 판단하고 ISR 그룹에서 추방한다. 즉 뒤처지지 않고 잘 복제하고 있는 팔로워들만 ISR 그룹에 속하게 되며, 리더에 장애가 발생한 경우 새로운 리더의 자격을 얻을 수 있다.
- ISR 내에서 모든 팔로워의 복제가 완료되면 리더는 내부적으로 커밋되었다는 표시를 하게된다. 여기서 마지막 커밋 오프셋 위치는 하이워터마크 라고 부른다. 커밋되었다라는 말은 모든 리플리케이션이 전부 메시지를 저장했음을 의미하고 커밋된 메시지만 컨슈머가 읽어갈 수 있다. 커밋된 메시지만 컨슈머가 읽을 수 있는 이유는 데이터의 일관성을 유지하기 위해서다.
  - 만약 커밋되기 전 메시지를 컨슈머가 읽을 수 있다고 가정하자.
    - 데이터 "message 1"이 커밋되었다.
    - 새로운 데이터 "message 2"가 리더에게 복제되었다. 이때 컨슈머 A는 리더로부터 새로운 데이터를 컨슘한 상태이며 아직 복제는 일어나지 않았다.
    - 이때 리더가 있는 브로커에 문제가 발생해 팔로워가 리더가 된다.
    - 컨슈머 B가 새로 붙었는데 "message 2"는 소실되어 "message 1"을 받게 된다.
  - 결과적으로 동일한 토픽의 파티션을 읽었지만 메시지가 일치하지 않는 현상이 발생하게 되므로, 커밋된 메시지만 컨슈머가 읽어갈 수 있도록 구현되어 있다.
- 모든 브로커는 시작될 때 커밋된 메시지를 유지하기 위해 로컬 디스크의 replication-offset-checkpoint 라는 파일에 마지막 커밋 오프셋 위치를 저장한다.

#### 4.1.4 리더와 팔로워의 단계별 리플리케이션 동작

- 수 많은 메시지를 읽고 쓰기 처리하는 리더가 복제 과정에서 많은 관여를 하면 리더의 성능이 떨어지게 된다. 따라서 카프카는 리더와 팔로워 간의 복제 과정에서 서로의 통신을 최소화 할 수 있도록 설계하여 리더의 부하를 줄였는데 이 과정을 알아보자.
  - 3개의 브로커가 있고 리더에게 첫 번째 데이터가 도착했고 팔로워가 리더에게 0번 오프셋 메시지 가져오기 요청을 보낸다.
  - 팔로워들은 새로운 메시지가 있다는 사실을 인지하고 메시지를 복제해간다. 현 상태에서 리더는 모든 팔로워가 0번 오프셋 메시지를 요청했다는 사실을 알지만, 0번 오프셋 복제가 성공했는지 알 수는 없다.
    - 전통적인 래빗MQ의 트랜잭션 모드에서는 모든 미러가 ACK를 리더에게 리턴하여 리더가 미러들이 메시지를 받았는지 알 수 있다. 하지만 카프카는 리더와 팔로워 사이에서 주고받는 ACK 통신이 없다.
  - 리더에게 두 번째 메시지가 도착했고 팔로워들은 1번 오프셋에 대한 복제를 요청한다. 팔로워들로부터 1번 오프셋 복제 요청을 받은 리더는 팔로워들의 0번 오프셋에 대한 복제가 성공했음을 인지하고 오프셋 0에 대한 커밋을 표시하고 하이워터마크를 증가시킨다. 만약 팔로워가 0번 오프셋에 대한 복제를 성공하지 못했다면 팔로워는 0번 오프셋에 대한 요청을 보내게 된다. 따라서 리더는 팔로워들이 보내는 복제 요청의 오프셋을 보고 팔로워들이 어느 위치의 오프셋까지 성공했는지 인지 할 수 있다.
  - 복제 요청을 받은 리더는 응답에 0번 오프셋 데이터가 커밋되었다는 내용도 전달한다. 응답을 받은 팔로워들은 0번 오프셋 메시지가 커밋되었다는 사실을 인지하고 리더와 동일하게 커밋을 표시한다. 그리고 1번 오프셋 메시지를 복제해 간다. 리더와 팔로워들은 이 과정을 반복적으로 수행해 복제를 이어간다.
- 중요한 사실은 다른 메시징 시스템은 리더와 팔로워가 메시지를 잘 받았는지 ACK 통신을 하지만 카프카는 통신 단계에서 제거했다. 대량의 메시지를 처리할 경우 이런 작은 차이도 크게 부각이 된다.
- 카프카에서 리더와 팔로워들의 복제 방식은 리더가 푸시하는 방식이 아니라 팔로워들이 풀하는 방식으로 동작하는데 이는 복제 과정에서 리더의 부하를 줄여주기 위해서다.

#### 4.1.5 리더 에포크와 복구

- 리더 에포크(LeaderEpoch)는 카프카 파티션들이 복구할 때 메시지 일관성을 위해 사용된다. 리더에포크는 컨트롤러에 의해 관리되며 복구 동작시 하이워터마크를 대체하는 수단으로 활용된다. 우선 리더에포크가 없을 때 메시지 일관성이 깨지는 경우를 보자.
  - 리더와 팔로워 파티션에 0번 오프셋에 데이터가 정상적으로 저장된다.
  - 리더는 팔로워 파티션의 1번 오프셋 요청에 하이워터마크를 1로 올린다.
  - 리더가 프로듀서의 두 번째 메시지를 받은 뒤 1번 오프셋에 저장하고 팔로워는 1번 오프셋의 메시지를 리더로부터 복제해간다.
  - 팔로워는 복제 후 2번 오프셋에 대한 요청을 리더에게 보내고, 리더는 하이워터마크를 2로 올린다.
  - 팔로워는 1번 오프셋을 복제했지만 아직 리더로부터 하이워터마크를 2로 올리는 내용을 전달받지 못한 상태이다.
  - 예상치 못한 장애로 팔로워가 다운된다.
  - 팔로워 장애가 해결되 재시작되면 자신의 하이워터마크보다 높은 메시지는 신뢰할 수 없어 삭제하게 된다.
  - 팔로워는 리더에게 1번 오프셋 데이터를 요청하고 이 순간 리더가 장애로 다운되면서 팔로워가 새로운 리더로 승격된다.
  - 결국 기존에 1번 오프셋에 쓰여진 두 번째 데이터는 손실된다.
- 리더에포크를 사용한다면?
  - 리더에포크를 사용하면 하이워터마크보다 앞에 있는 메시지를 삭제하지 않고 리더에게 리더에포크 요청을 보낸다.
  - 리더는 1번 오프셋까지 쓰여졌다라고 보내면 팔로워는 1번 오프셋을 삭제하지 않고 자신의 하이워터마크를 상향조정한다.

### 4.2 컨트롤러

- 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 맡게 되며 ISR 리스트 중에 리더를 선출한다. 리더를 선출하기 위한 ISR 정보는 안전한 저장소에 보관되어야 하는데 주키퍼에 저장되어 있다.
- 컨트롤러는 브로커가 실패하는 것을 감시하다가 실패가 감지되면 ISR 리스트 중 하나를 새로운 파티션 리더로 선출한다. 그 후 새로운 리더 정보를 주키퍼에 기록하고 변경된 정보를 모든 브로커에게 전달한다.
- 리더가 다운됐다는 것은 프로듀서 / 컨슈머의 작업이 실패하게 되며, 설정된 재시도 숫자만큼 재시도를 하게 된다. 따라서 클라이언트들이 재시도 하는 동안 리더 선출 작업이 빠르게 이뤄져야 한다.
- 리더 선출 과정
  - 1,3번 브로커에 0번 파티션의 리더/팔로워가 있고 리더 브로커인 1번에 장애가 발생한다.
  - 주키퍼는 브로커와 연결이 끊어진 후 0번 파티션의 ISR 변화를 감지한다.
  - 컨트롤러는 주키퍼 워치를 통해 0번 파티션에 변화가 생긴 것을 감지하고 ISR중 남아있는 팔로워를 리더로 선출한다.
  - 컨트롤러는 0번 파티션의 새로운 리더가 3번 이라는 정보를 주키퍼에 기록하고 모든 브로커에게 전파된다.
- 파티션이 1개일 경우 리더 선출 작업은 매우 빠르게 이뤄지지만 파티션이 많을 수록 리더 선출 작업이 느려진다. 이를 개선하기 위한 연구 끝에 카프카 버전 1.0.0 에서 6분 30초가 소요되던 작업이 1.1.0에서는 3초만에 완료되어 이런 문제에 안심할 수 있게 되었다.
- 위 경우는 예기치 않은 장애로 인한 리더 선출과정을 설명했는데, 제어된 종료 과정에서 리더 선출 작업을 살펴보자. 제어된 브로커 종료란 관리자에 의해 자연스러운 종료를 생각하면 된다.
- 제어된 종료와 급작스러운 종료의 차이는 다운타임이다. 제어된 종료를 사용하면 컨트롤러가 해당 브로커가 리더로 할당된 전체 파티션에 대해 리더 선출 작업을 진행하기 때문에 다운타임을 최소화 할 수 있다. 또한 제어된 종료의 경우 브로커는 자신의 모든 로그를 디스크에 동기화 한 후에 종료되므로 재시작할 때 로그 복구 시간이 짧다. 다양한 장점이 있는 제어된 종료를 사용하려면 controlled.shutdown.enable이 true로 브로커 설정 파일인 server.properties에 적용되어야 한다.

### 4.3 로그 (로그 세그먼트)

- 카프카에 저장되는 레코드는 세그먼트라는 파일에 순차적으로 저장된다. 로그 세그먼트 크기가 너무 커져버리면 파일을 관리하기 어렵기 때문에 최대 크기는 1GB를 기본값으로 설정되어 있다. 로그 세그먼트가 1GB 보다 커지면 롤링 전략을 사용하는데 레코드를 세그먼트 파일에 계속 덧붙이다가 크기가 1GB에 도달하면 해당 세그먼트 파일을 close하고 새로운 로그 세그먼트를 생성한다.
- 세그먼트 파일이 무한히 늘어날 경우를 대비해 카프카에서는 세그먼트 삭제와 컴팩션을 지원한다.

#### 4.3.1 로그 세그먼트 삭제

- 카프카 관리자는 토픽에 별도로 retention.ms 옵션을 설정해 보관기간을 설정할 수 있고, 각 로그 세그먼트는 보관기간이 지나면 삭제가 된다. 또한 retention.bytes 옵션으로 지정된 크기를 기준으로도 삭제가 가능하다.

#### 4.3.2 로그 세그먼트 컴팩션

- 카프카의 컴팩션은 일반 압축과는 다르게 키 값을 기준으로 마지막 데이터만 보관하게 된다.
- 예를 들어 세그먼트에 키가 k1인 밸류로 v1, v2, v3가 저장되어 있다고 가정하자. 컴팩션이 발생하면 이 세그먼트에는 키가 k1이고 밸류가 v3만 남게 되는 것이다. 이처럼 로그 컴팩션은 키를 기준으로 과거 정보는 중요하지 않고 가장 마지막 값이 필요한 경우에만 사용한다.
  <img src="https://github.com/programmer-sjk/TIL/blob/main/images/books/devops/log-compaction.png" width="500">

- 로그 컴팩션의 다른 장점으로는 빠른 장애 복구이다. 장애 복구 시 전체 로그를 복구하지 않고 메시지 키를 기준으로 최신의 상태만 복구한다. 하지만 로그 컴팩션은 키 값을 기준으로 최종값만 필요한 토픽에 적용해야 하며 모든 토픽에 적용하는 것은 좋지 않다.

## 5. 프로듀서의 내부 동작원리와 구현

### 5.1 파티셔너

- 프로듀서는 토픽으로 메시지를 보낼 때 어떤 파티션으로 메시지를 보낼지 결정해야 하는데 이때 사용하는 것이 파티셔너다.
- 많은 메시지가 카프카로 인입되는 경우 카프카는 처리량을 늘리기 위해 파티션을 늘릴 수 있는 기능을 제공한다. 이 때 메시지 키와 매핑된 해시 테이블도 변경되어 동일한 키를 사용해 메시지를 전송하더라도 파티션을 늘린 후에는 다른 파티션으로 전송될 수 있다. 따라서 되도록 파티션 수를 변경하지 않는 것을 권장한다.

#### 5.1.1 라운드 로빈 전략

- 메시지 키가 없다면 라운드 로빈 방식으로 파티션에게 전달되는데, 이는 배치 처리 과정에서 비효율적인 면이 있다.
- 프로듀서가 키가 없는 메시지를 보내고 파티셔너는 어떤 파티션으로 보낼지 정해서 메모리에 담아둔다 (배치 처리). 이렇게 레코드가 파티셔너를 거쳐 지나갔지만 파티션 별 최소 레코드 수를 충족하지 못하면 계속 대기하게 된다. 예를 들어 한 파티션에 3개 레코드를 충족해야 카프카에게 보내는데 2개만 들어올 경우 계속 대기하는 경우다.
- 물론 관리자가 특정시간 초과 시 카프카로 보내도록 설정할 수 있지만 배치/압축 효과를 얻지 못하고 하나만 카프카로 전송되므로 비효율적이다. 카프카에서는 이를 보완하기 위해 스티키 파티셔닝을 공개했다.

#### 5.1.2 스티키 파티셔닝 전략

- 2019년 카프카 2.4 버전부터는 스티키 파티셔닝 전략을 사용한다. 스티키 파티셔닝이란 하나의 파티션에 레코드 수를 먼저 채워서 배치 전송하는 전략을 의미한다. 즉 키가 없다면 파티셔너가 라운드 로빈으로 파티션을 정해 버퍼에 두는게 아니라 하나의 파티션을 정해 레코드를 담아두고 배치 전송을 수행한다.
- 컨플루언트에 따르면 스티키 파티셔닝 전략을 적용해 기본 설정에 비해 30% 이상 지연시간이 감소하고 프로듀서 CPU 사용률도 감소하는 효과가 있으니, 메시지 순서가 중요하지 않다면 스티키 파티셔닝 전략을 사용하자.

### 5.2 프로듀서의 배치

- 프로듀서는 배치 전송을 위해 아래 옵션을 제공한다.
  - buffer.memory: 메시지 전송을 위해 담아두는 프로듀서의 버퍼 메모리. 기본값은 32MB
  - batch.size: 배치 전송을 위해 레코드를 묶는 단위를 설정한다. 기본값 16KB
  - linger.ms: 배치 전송을 위해 메모리에서 대기하는 최대 대기시간. 기본값은 0이며 배치 전송 없이 바로 전달을 의미
- 배치 전송은 불필요한 I/O를 줄일 수 있는 장점이 있지만 카프카를 사용하는 목적에 따라 처리량을 높일지 지연 없는 전송을 해야할지 결정해야 한다. 처리량을 높이려면 batch.size, linger.ms 값을 크게 설정해야 하고, 지연 없는 전송이 목표라면
  두 옵션 값을 작게 설정해야 한다.

### 5.3 중복 없는 전송

- 카프카는 중복없이 전송할 수 있는 기능(멱등성)을 제공한다. 이 부분을 알아보기 전에 메시지 시스템들의 전송 방식을 살펴보자. 메시지 시스템들의 전송 방식에는 `적어도 한 번 전송(at-least-once)`, `최대 한 번 전송(at-most-once)`,
  `정확히 한 번 전송(exactly-once)` 들이 있다.

#### 적어도 한 번 전송

- 통신 절차는 아래처럼 동작한다.
  - 프로듀서가 브로커로 메시지 전송
  - 브로커가 전달받고 ACK를 보냈는데 유실됨
  - 프로듀서는 ACK를 못 받았기 때문에 재전송한다.
- 위 과정에서 브로커가 처음 데이터를 정상 저장했냐 여부에 따라 한 번만 저장되거나 중복 저장이된다.
- 이처럼 일부 메시지 중복이 발생할 수는 있지만 최소한 하나의 메시지는 보장하는 방식이며 카프카는 이 방식으로 기본 동작한다.

#### 최대 한번 전송

- 통신 절차는 아래처럼 동작한다.
  - 프로듀서가 브로커로 메시지 전송
  - 브로커가 전달받고 ACK를 보냈는데 유실됨
  - 프로듀서는 ACK 여부와 상관없이 다음 데이터를 전송한다.
- 즉 일부 손실을 감안하더라도 중복 전송은 하지 않는 경우다. 일부 메시지가 손실되더라도 높은 처리량을 요구하는 대량의 로그 수집이나 IoT 환경에서 사용되곤 한다.

#### 중복없는 전송

- 카프카 0.11 버전에서 추가된 기능으로 기본적으로 정확히 한 번 전송이랑 과정은 동일하다.
- 프로듀서는 브로커에 메시지를 전달할 때 Producer ID와 메시지 번호(시퀀스 번호)를 헤더에 포함해 함께 전달한다.
- 브로커는 각 메시지마다 PID 값과 시퀀스 번호를 메모리에 유지하게 되며 이 정보를 이용해 프로듀서가 동일한 메시지를 재전송 하더라도 브로커에는 메시지가 중복 저장되지 않는다.
- PID와 메시지 번호는 브로커의 메모리에 유지되고 복제될 떄 로그에도 저장된다. 따라서 예상치 못한 브로커 장애로 리더가 변경되더라도 새로운 리더가 PID와 시퀀스 번호를 알고 있어 중복없이 메시지 전송이 가능하다.
- 결국 중복을 피하기 위해 오버헤드가 존재하긴 하지만 카프카에서는 메시지에 단순한 숫자 필드만 추가하는 방법으로 구현했기 때문에 오버헤드가 높지는 않다. 컨플루언트에 따르면 중복 없는 전송을 적용한 후 20%의 성능 감소가 발생했다고 한다. 따라서 성능이 민감하지 않은 상황에서 중복 없는 메시지 전송이 필요하다면 이 방식을 적용해야 한다.
- 중복 없는 전송을 위해 프로듀서 설정 값들은 아래와 같다.
  - enable.idempotence: true로 설정
  - max.in.flight.requests.per.connection: 기본 값은 5이며, 5 이하로 설정해야 한다.
  - acks: all로 설정해야 한다.
  - retries: ACK를 못 받은 경우 재시도 해야 하므로 0보다 큰 값으로 설정
- 브로커가 저장하는 PID와 시퀀스 번호는 카프카 로그가 저장되는 경로의 snapshot 파일을 통해 확인할 수 있다.

### 5.4 정확히 한 번 전송

- 앞에서 중복 없는 전송을 할 수 있다고 했는데, 이 방식이 정확히 한 번 전송한다는 의미는 아니다. 카프카에서 정확히 한 번 전송은 트랜잭션과 같은 전체적인 프로세스 처리를 의미하며 중복 없는 전송은 정확히 한 번 전송의 일부 기능이라고 할 수 있다.

#### 5.4.1 디자인

- 프로듀서가 카프카로 정확히 한 번 전송 방식으로 메시지를 전송할 때 카프카에는 트랜잭션 코디네이터 라는게 존재한다. 트랜잭션 코디네이터의 역할은 프로듀서가 전송한 메시지를 관리하며 커밋 또는 중단을 표시한다.
- 트랜잭션 로그를 카프카의 내부 토픽인 `_transaction_state`에 저장한다.
- 중복 없는 전송과 달리 옵션 설정에서 가장큰 차이점은 TRANSACTIONAL_IF_CONFIG 옵션이다. 이 옵션을 실행하는 프로듀서 프로세스 마다 고유한 아이디로 설정해야 한다.

#### 5.4.3 단계별 동작

- 정확히 한번 전송 방식을 위해서는 트랜잭션 API를 이용한다. 따라서 프로듀서는 가장 먼저 트랜잭션 코디네이터를 찾는다. 트랜잭션 코디네이터는 Producer ID와 transaction.id를 매핑해서 해당 트랜잭션 전체를 관리한다.
- 그 다음 프로듀서는 PID, TID를 트랜잭션 코디네이터에게 보내고 트랜잭션 코디네이터는 TID, PID를 매핑해서 해당 정보를 트랜잭션 로그에 기록한다.
- 프로듀서는 트랜잭션을 시작하겠다는 메소드를 호출하고 토픽 파티션 정보를 트랜잭션 코디네이터에게 전달한다. 트랜잭션 코디네이터는 트랜잭션 상태를 Ongoing으로 표시한다.
- 프로듀서는 메시지를 전달하는데, 이 메시지에는 PID, 에포크, 시퀀스 번호가 함께 포함되어 전송한다. 메시지 전송을 완료한 프로듀서는 commitTransaction 같은 메소드를 호출해 트랜잭션이 완료됨을 트랜잭션 코디네이터에게 알린다.
- 트랜잭션 코디네이터는 트랜잭션 로그에 기록된 토픽의 파티션에 트랜잭션 커밋 표시를 하고 프로듀서에게 트랜잭션이 완료됨을 알린며 처리를 마무리 한다. 트랜잭션을 이용하는 컨슈머는 read_commited 설정을 하면 트랜잭션에 성공한 메시지들만 읽을 수 있게 된다.

## 6. 컨슈머의 내부 동작원리와 구현

### 6.1 컨슈머 오프셋 관리

- 컨슈머 그룹은 토픽의 메시지를 읽은 뒤, 읽어온 위치의 오프셋 정보를 카프카의 \_consumer_offsets 토픽에 저장한다. 이 토픽에 기록된 정보를 이용해 컨슈머 그룹은 자신에게 속한 컨슈머의 변경(컨슈머 장애 or 컨슈머 이탈)이 발생하는 경우 해당 컨슈머가 어느 위치까지 읽었는지 추적할 수 있다.

### 6.2 그룹 코디네이터

- 컨슈머 그룹 내에서 컨슈머들이 그룹을 떠날 수도 있고 새로운 컨슈머가 합류할 수도 있으므로 컨슈머 그룹은 이러한 변화를 인지하고 각 컨슈머들에게 작업을 분배해야 한다. 이걸 컨슈머 리밸런싱이라고 부른다.
- 컨슈머 그룹 관리를 위해 별도의 코디네이터가 존재하는 이를 그룹 코디네이터라 부른다. 컨슈머 그룹별로 그룹 코디네이터가 존재하게 되며 컨슈머 그룹이 브로커에 최초 연결 요청을 보내면 브로커 중 하나에 그룹 코디네이터가 생성된다.
- 컨슈머 그룹 등록 과정에서 그룹 코디네이터와 동작하는 과정을 보자.
  - 컨슈머는 brokers 리스트에 있는 브로커에게 연결 요청을 보낸다.
  - 요청을 받은 브로커는 그룹 코디네이터를 생성하고 컨슈머에게 응답을 보낸다.
  - 컨슈머는 컨슈머 등록 요청을 그룹 코디네이터에게 보낸다. 이때 가장 먼저 요청을 보내는 컨슈머가 그룹의 리더가 된다.
  - 컨슈머 등록 요청을 받은 코디네이터는 컨슈머 그룹이 구독하는 토픽, 파티션 리스트를 리더에게 응답보낸다.
  - 리더 컨슈머는 그룹 내 컨슈머에게 파티션을 할당한 뒤 그룹 코디네이터에게 전달한다.
  - 그룹 코디네이터는 해당 정보를 캐시하고 각 그룹내 컨슈머들에게 성공을 알린다.
  - 각 컨슈머들은 지정된 파티션으로부터 메시지를 가져온다.
- 컨슈머들의 변경을 감지하기 위해 컨슈머들과 그룹 코디네이터는 서로 하트비트를 주고받는다. 하트비트를 주기적으로 주고 받으면서 그룹 코디네이터는 컨슈머의 정상 동작을 체크하고 문제가 있다고 판단되면 컨슈머 리밸런싱을 통해 컨슈머 그룹의 전체 균형을 다시 맞춘다.

### 6.3 스태틱 멤버십

- 떄로 소프트웨어 업데이트 등의 이유로 관리자는 컨슈머 그룹내의 컨슈머들을 하나씩 재시작하고 싶을 때가 있을 것이다. 하지만 컨슈머가 재 시작될 때마다 리밸런싱이 일어나고, 리밸런싱 작업이 일어나는 동안 컨슈머들은 일시 중지하므로 매우 번거로운 일이다.
- 컨슈머 그룹에는 각 컨슈머를 식별하기 위한 ID가 부여되는데 컨슈머가 재 시작되면 동일한 컨슈머임에도 새로운 ID가 부여되기 때문에 컨슈머 그룹의 리밸런싱이 발생한다. 이때 컨슈머가 그룹에서 떠날때와 다시 합류할 때 일어나므로 총 2번의 리밸런싱이 발생한다.
- 리밸런싱이 발생하면 해당 컨슈머를 대상으로 하지 않고 컨슈머 그룹의 전체 컨슈머를 대상으로 리밸런싱이 발생하기 때문에 부담이 큰 고 비용 작업이다.
- 카프카는 불필요한 리밸런싱을 방지하기 위해 카프카 2.3 버전부터 스태틱 멤버십을 도입해, 컨슈머 그룹내에서 컨슈머가 재시작 등으로 그룹에서 나갔다가 다시 합류해도 리밸런싱이 일어나지 않게 설정할 수 있다.

### 6.4 컨슈머 파티션 할당 전략

- 컨슈머 그룹의 리더는 정해진 파티션 할당 전략에 따라 컨슈머와 토픽의 파티션을 매칭시킨다. 각 할당전략은 아래와 같다.
  - 레인지: 기본값으로 토픽별로 할당
  - 라운도 로빈: 파티션과 컨슈머들을 라운드 로빈으로 할당
  - 스티키: 컨슈머가 컨슘하는 파티션을 계속 유지할 수 있음
  - 협력적 스티키: 스티키와 유사하지만 전체 일시 정지가 아닌 연속적 재조정 방식

#### 6.4.3 스티키 파티션 할당 전략

- 레인지와 라운드 로빈 파티션 할당 전략은 리밸런싱이 발생하면 파티션과 컨슈머가 똑같이 매핑되지 않을 수 있다. 따라서 리밸런싱이 발생해도 기존에 매핑된 파티션과 컨슈머를 최대한 유지하려는 전략이 스티키 파티션 할당 전략이다.
- 이 때 균형잡힌 파티션 할당도 중요하므로 스티키 전략이라고 무조건 기존의 파티션과 컨슈머를 유지하지는 않는다.
- 각 파티션이 있고, 컨슈머 그룹에 3개의 컨슈머가 있다고 가정하고 이 중 2번 컨슈머가 빠졌다고 가정하자.
  - 만약 라운드 로빈 전략으로 리밸런싱이 발생한다고 가정하자. 그러면 파티션과 컨슈머를 순서대로 배치해서 라운드 로빈 전략에 맞춰 하나씩 매핑하게 된다.
  - 만약 스티키 전략으로 리밸런싱이 발생한다면, 존재하는 컨슈머 1,3에 매핑된 파티션은 그대로 유지되며 컨슈머 2에 할당된 파티션들만 컨슈머 1,3에 분배되어 할당된다.
- 스티키 파티션 전략은 최대한 컨슈머들의 균형을 맞추고 기존 컨슈머에 할당된 파티션을 유지하는 방향으로 라운드 로빈 방식보다 효율적ㅈ이다.

#### 6.4.4 협력적 스티키 파티션 할당 전략

- 협력적 스티키 방식과 스티키 방식의 차이는 컨슈머 그룹 내부의 리밸런싱 동작이 한층 고도화 됐다는 점이다.
- 지금까지 컨슈러 리밸런싱 동작에는 내부적으로 EAGER 프로토콜을 사용했고, 이 동작은 리밸런싱때 컨슈머에 할당 된 모든 파티션을 취소하고 컨슈머 그룹 전체에서 컨슈머와 파티션을 재 할당했다. 모든 매핑이 취소되고 재 할당되는 동안 컨슈머의 다운타임이 발생하기 때문에 큰 부담이 존재한다.
- 카프카 2.3 버전부터는 협력적 스티키 할당 전략이 적용됐고, EAGER가 아닌 COOPERATIVE 프토콜을 적용했다. 이 프로토콜은 파티션 재 배치가 필요하지 않은 컨슈머들은 다운타임 없이 계속 동작한다는 점이다.

### 6.5 정확히 한 번 컨슈머 동작

- 프로듀서에서 정확히 한 번 전송을 위해 별도의 트랜잭션 코디네이터가 필요했다. 트랜잭션 코디네이터는 프로듀서의 정확히 한 번 전송이 성공하면 해당 레코드의 트랜잭션 성공을 표시하는 특수한 메시지를 추가한다. 따라서 컨슈머는 트랜잭션 코디네이터가 표시한 메시지만 읽는다면, 정확히 한 번 읽을 수 있다.
- 일반 컨슈머 코드에서 ISOLATION_LEVEL_CONFIG 설정만 추가하면 트랜잭션 컨슈머로 동작하게 된다. 기본 값은
  read_uncommited로 모든 메시지를 읽을 수 있다는 뜻이며 read_commited로 변경하면 트랜잭션이 완료된 메시지만 읽을 수 있게 된다.
- 사용자들이 트랜잭션 컨슈머가 정확히 한 번만 가져오는 것으로 오해를 많이 한다. 프로듀서의 경우 트랜잭션 코디네이터와 통신하면서 해당 트랜잭션이 정확하게 처리하는 것을 보장했지만 컨슈머의 경우 옵션으로 트랜잭션 프로듀서가 보낸 메시지만 가져올 수 있다. 컨슈머는 트랜잭션 코디네이터와 통신하는 부분이 없으므로 정확하게 메시지를 한 번 가져오는지는 보장할 수 없다.
- 일부 컨슈머 어플리케이션은 정확히 한 번을 지원하는 경우가 있으므로 사용하려는 컨슈머 어플리케이션 가이드 문서를 잘 읽어보고 적용해야 한다.

## 7. 카프카 운영과 모니터링

### 7.1 안정적인 운영을 위한 주키퍼와 카프카 구성

- 카프카에서 주키퍼 의존성은 곧 제거될 것으로 보인다.
- 주키퍼는 기본적으로 홀수로 구성해야 하며 카프카는 주키퍼와 다르게 반드시 홀수일 필요는 없지만, 카프카에서 추천하는 안정적인 브로커의 개수는 3대가 적당하다.

### 7.2 모니터링 시스템 구성

- 카프카 모니터링을 위해 프로메테우스, 그라파나, 익스포터를 설치하고 실습하는 파트지만 굳이 정리하진 않는다.

## 8. 카프카 버전 업그레이드와 확장

### 8.1 카프카 버전 업그레이드를 위한 준비

- 현재 사용하는 카프카의 버전과 설치할 카프카의 버전을 알아야 한다. 사용중인 카프카의 버전은 아래 명령어로 알 수 있다.
  - `kafka-topic.sh --version`
- 업그레이드 하려는 버전의 릴리즈 노트를 보면서 버전 업그레이드시 문제가 없는지 확인해야 한다.
- 만약 카프카가 다운타임을 가질 수 있다면 현재 버전의 카프카를 모두 종료하고 최신 버전의 카프카를 실행하면 되므로 매우 간단하게 업그레이드가 완료된다. 하지만 다운타임을 가질 수 없다면 브로커 한 대씩 롤링 업그레이드를 해야 한다.

### 업그레이드 작업 시 주의사항

- 실무에선 업그레이드 과정에서 문제가 발생할 수 있으니 추천하는 방법은, 실제 업그레이드 하기에 앞서서 운영 환경과 동일한 카프카 버전으로 개발용 카프카를 구성해보고 개발용 카파카의 버전 업그레이드를 수행하는 것이다.

## 9. 카프카 보안

- 카프카는 최초 설치 과정에서 보안이 설정되지 않으므로 클라이언트들이 자유롭게 카프카와 연결할 수 있어 보안의 문제가 있다.
- 카프카는 암호화, 인증, 권한이라는 세 가지 보안요소가 필요하다.
  - 암호화는 SSL 통신을 통해 제공
  - 인증은 다양한 프로토콜 중 하나를 채택하면 되고 보통 커버로스 인증 방식을 많이 사용한다.
  - 권한은 ACL을 적용하는 것으로 kafka-acls.sh 을 활용해 유저별 권한을 설정할 수 있다.

## 10. 스키마 레지스트리

### 10.1 스키마의 개념과 유용성

- 스키마란 정보를 구성하고 해석하는데 도움을 주는 개념으로 카프카에 스키마가 없다고 가정하면 누군가의 실수로 사전에 정의되지 않은 형태의 데이터를 토픽에 보내면, 토픽과 연결된 모든 시스템이 영향을 받을 수 있어 카프카에서 스키마 사용은 권장사항에 속한다.

### 10.2 카프카와 스키마 레지스트리

#### 10.2.1 스키마 레지스트리 개요

- 카프카에서 스키마를 활용하는 방법은 스키마 레지스트리라는 별도의 어플리케이션을 이용하는 것이다. 스키마 레지스트리는 컨플루언트 커뮤니티 라이선스를 가지고 있는데 비상업적 용도에 한해 무료로 사용할 수 있다.
  <img src="https://github.com/programmer-sjk/TIL/blob/main/images/books/devops/schema-registry.png" width="500">

- 위 그림에서 알 수 있듯이 스키마 레지스트리는 카프카와 별도로 구성된 어플리케이션이다. 클라이언트들이 스키마 정보를 사용하기 위해서는 프로듀서, 스키마 레지스트리, 컨슈머간 직접 통신이 이뤄져야 한다.
- 프로듀서는 스키마 레지스트리에 스키마를 등록하고 스키마 레지스트리는 프로듀서에 의해 등록된 스키마 정보를 카프카의 내부 토픽에 저장한다. 프로듀서는 스키마 레지스트리에 등록된 스키마의 ID와 메시지를 카프카로 전송하고 컨슈머는 스키마 ID를 스키마 레지스트리로부터 읽어온 후 프로듀서가 전송한 스키마 ID와 메시지를 조합해 읽을 수 있다.
- 스키마 레지스트리를 처음 접하면 필요성에 의구심이 들 수 있다. 하지만 확장성과 방향성, 메시지 변화에 따른 유연성을 고려한다면 스키마 레지스트리는 카프카에 필요한 어플리케이션이다. 스키마 정의는 번거롭지만, 한 번 스키마를 정의해 놓으면 이후 스키마 변경에 유연하게 대처할 수 있다.

## 11. 카프카 커넥트

- 카프카 커넥트는 아파치 카프카의 오픈소스 프로젝트 중 하나로 DB 같은 외부 시스템과 카프카를 손쉽게 연결하기 위한 프레임워크다.
- 프로듀서나 컨슈머를 직접 개발해 원하는 동작을 실행하고 처리할 수 있지만 떄로 어플리케이션 개발과 운영 비용이 부담되는 경우 카프카 커넥터를 이용하면 효율적이고 빠르게 클라이언트를 구성하고 적용할 수 있다.
- 카프카 커넥트는 카프카 클러스터를 먼저 구성한 후 카프카 클러스터의 양쪽 옆에 배치할 수 있다. 소스 방향에 있는 커넥트를 소스 커넥트, 싱크 방향에 있는 커넥트를 싱크 커넥트라고 부른다. 카프카로 설명하면 프로듀서의 역할을 하는게 소스 커넥터이고 컨슈머 역할을 하는게 싱크 커넥트이다.
- 카프카 커넥트는 단독모드와 분산모드로 실행할 수 있으며, 운영 환경에서는 분산모드를 사용하는게 안정적이다.

## 12. 엔터프라이즈 카프카 아키텍처 구성 사례

- 실무에서는 장애 복구를 위해 하나 이상의 데이터 센터를 운영하는 경우가 많다. 따라서 카프카 클러스터도 여러 데이터 센터에 배치되므로 카프카 간에 리플리케이션은 필수이고, 리플리케이션에 주로 미러 메이커를 사용한다.
- 여러 데이터 센터에 있는 카프카의 데이터를 모으기 위해 리플리케이션을 사용하기도 한다. 중앙에 있는 카프카 클러스터로 합쳐진 데이터는 실시간 처리와 분석을 위해 하둡이나 ES, S3, HBase 같은 곳에 적재된다.
- 그 후에 내용은 실습 내용이라 여기서는 정리하지 않는다.

## 13. 카프카의 발전과 미래

### 13.1 주키퍼 없는 카프카 미래

- 처음 카프카에서는 주키퍼를 분산 코디네이터로 이용함에 별다른 이슈가 없었지만, 카프카가 예상보다 많은 메시지를 처리하면서 운영이나 보안, 의존성의 문제들이 발생하였다. 결국 카프카 진영에서는 주키퍼에 대한 의존성을 제거하길 원해왔으며 현재는 주키퍼가 제거된 카프카가 릴리즈 된 상황이다.

### 13.2 새로운 합의 프로토콜 KRaft

- 기존에 주키퍼 의존성이 있을 때 주키퍼 합의 프로토콜을 이용해 리더 선출 작업을 해왔다. 하지만 주키퍼 의존성을 제거하면서 새로운 합의 프로토콜이 필요해졌으며 래프트 합의 프로토콜과 카프카의 특성을 합해 크래프트 합의 프로토콜을 개발했다. 크래프트 모드를 사용하면 메타데이터를 카프카에서 직접 관리하고 주기적으로 스냅샷을 생성한다. 장애 발생시 전체 데이터를 복구하지 않고 스냅샷에서 필요한 부분만 복구하므로 획기적으로 복구 시간이 단축될 것으로 보인다.

### 13.4 카프카의 미래가 담긴 KIP

- KIP(Kafka Improvement Proposals) 사이트를 통해 카프카의 성장이나 미래의 모습을 확인할 수 있다.

# 아파치 카프카 애플리케이션 프로그래밍 with 자바

- [책 링크](https://www.yes24.com/Product/Goods/99122569)

## 1. 들어가며

### 1.1 카프카의 탄생

- 링크드인은 초기에 단방향 통신을 통해 소스 어플리케이션에서 타겟 어플리케이션으로 연동하는 소스 코드를 작성하여 운영했다. 시간이 지날수록 아키텍처는 복잡해졌고 소스/타겟 어플리케이션이 많아지면서 데이터를 전송하는 라인이 복잡해지기 시작했다.
- 링크드인의 데이터 팀은 신규 시스템을 만들기로 결정했고 그 결과물이 아파치 카프카다. 각 어플리케이션이 연결하여 데이터를 처리하는 것이 아니라 한 곳에 모아 처리할 수 있도록 중앙집중화했다. 웹 사이트, 어플리케이션, 센서에서 취합한 데이터 스트림을 카프카 한 곳에서 수집하고 사용자들이 실시간으로 소비할 수 있게 동작한다.
- 기존에 1:1 매칭으로 운영하던 시스템에선 한쪽의 장애가 한쪽에 영향을 미치곤 했지만 카프카는 이러한 의존도를 타파한다. 소스 어플리케이션에서 생성되는 데이터는 어떤 타겟 어플리케이션으로 보낼 것인지 고민하지 않고 카프카로 넣으면 된다. 카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 방식의 큐와 유사하다. 큐에 데이터를 보내는 것이 프로듀서고, 큐에서 데이터를 가져가는 것이 컨슈머다.
- 상용 환경에서 카프카는 3대 이상의 브로커에서 분산 운영하여 데이터를 안전하게 기록한다. 3대 이상으로 이루어진 카프카 클러스터 중 일부 서버에 장애가 발생하더라도 데이터를 지속적으로 복제하기 때문에 안전하게 운영할 수 있다.

### 1.2 빅데이터 파이프라인에서 카프카 역할

- 빅 데이터를 저장하고 활용하기 위해, 우선 생성되는 데이터를 모두 모으는 것이 중요한데 이때 사용되는 개념이 데이터 레이크다. 데이터 웨어 하우스와는 다르게 필터링되거나 패키지화 되지 않은 데이터가 저장된다는 점이 특징이다.
- 서비스에서 발생하는 데이터를 데이터 레이크에 모으려면 단순히 생각할 때 발생하는 데이터를 직접 end-to-end 방식으로 넣을 수 있다. 하지만 서비스가 커지고 복잡해질수록 링크드인처럼 파편화되고 복잡도가 올라가는 문제가 발생한다. 이 때 데이터 파이프라인을 안정적이고 확장성 높게 운영하기 위해 좋은 방법 중 하나가 카프카를 활용하는 것이다. 왜 카프카가 적합한지 상세히 살펴보자.
  - 높은 처리량: 카프카는 프로듀서/컨슈머가 데이터를 보내고 받을 때 모두 묶어서 전송한다. 많은 양의 데이터를 묶음 단위로 배치에서 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그데이터를 처리하는데 적합하다.
  - 확장성: 카프카는 데이터 양이 가변적인 환경에서 안정적으로 확장 가능하도록 설계되었다. 데이터가 적을 때는 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면 브로커 개수를 늘려 스케일 아웃 할 수 있다.
  - 영속성: 영속성은 프로그램이 종료되더라도 사라지지 않는 데이터 특성을 뜻한다. 카프카는 다른 메시징 시스템과는 다르게 전송받은 데이터를 파일 시스템에 저장한다. 파일 시스템을 사용하는 것이 느리다고 생각하겠지만 카프카는 OS 레벨에서 파일 I/O 성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성해서 사용한다.
  - 고가용성: 3개 이상의 서버로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다.
- 카프카 클러스터를 3대 이상의 브로커들로 구성해야 하는 이유
  - 브로커를 1대만 유지한다면 테스트 목적으로만 사용한다.
  - 브로커를 2대로 유지하면 한 대의 브로커에 장애가 발생하더라도 한 대가 살아 있어 안정적으로 데이터를 처리할 수 있다. 하지만 브로커 간에 데이터가 복제되는 시간차이로 일부 데이터가 유실될 가능성이 있다. 예를 들면 leader가 데이터를 받고 follower가 복제를 해야 하는데 leader가 죽은 경우.
  - 유실을 막기 위해 min.insync.replicas 옵션을 사용할 수 있는데 2로 설정하면 최소 2개 이상의 브로커에 데이터가 완전히 복제됨을 보장한다. 이 옵션을 2로 설정하면 브로커를 3대 이상으로 운영해야 한다.
  - 정리하면 유실없이 데이터를 복제하기 위해 min.insync.replicas 값을 2로 설정해야 한다. 그래야 최소 2개 이상의 브로커에 데이터가 완전히 복제되니까. 근데 브로커가 2대이면 한 대가 죽을 경우 replicas 2보다 작은 한 대의 브로커만 받기 때문에 에러가 발생한다. 따라서 3대로 운영하고 min.insync.replicas 값이 2면 카프카 한 대가 죽어도 정상적으로 동작한다.

### 1.3 데이터 레이크 아키텍처와 카프카 미래

- 데이터 레이크를 구성하는 아키텍처는 크게 람다 아키텍처와 카파 아키텍처가 있다.
- 람다 아키텍처는 기존 e2e로 데이터를 수집하는 레거시를 개선하기 위해 구성된 아키텍처다. 배치 데이터를 일괄 처리하는 배치 레이어, 가공된 데이터를 제공하는 서빙 레이어, 실시간으로 데이터를 분석하는 스피드 레이어로 구성된다.
- 람다 아키텍처는 데이터를 배치 처리하는 레이어와 실시간 처리를 분담할 수 있었지만, 레이어가 2개로 나뉘기 때문에 생기는 단점들도 있었다. 제이 크랩스는 람다 아키텍처에서 배치 레이어를 제거한 카파 아키텍처를 제안했다.
- 카파 아키텍처에서는 스피드 레이어에서 데이터를 모두 처리할 수 있어 효율적으로 개발과 운영을 할 수 있었으나 서비스에서 생성되는 모든 종류의 데이터를 스트림 처리해야 했고, 서비스에서 생성된 모든 데이터가 스피드 레이어에 들어오는 것을 감안하면 내결함성과 장애 허용 특징을 지녀야 했다. 아파치 카프카는 이런 특성에 정확히 부합하는 플랫폼이다.

## 2. 카프카 빠르게 시작해보기

### 2.2.1 kafka-topic.sh

- 카프카 클러스터에는 토픽이 여러개 존재할 수 있다. 토픽에는 파티션이 존재하는데 파티션의 개수는 최소 1개부터 시작한다. 토픽을 생성하는 방법은 2가지가 있는데 아래와 같다.
  - 컨슈머, 프로듀서가 생성되지 않은 토픽에 데이터를 요청할 때
  - 커맨드라인 툴로 명시적으로 토픽을 생성할 때
- 토픽을 생성할 때는 명시적으로 생성하는 걸 추천한다. 토픽마다 처리되어야 하는 데이터 특성이 다르기 때문이다.
- 토픽 생성 예시
  - `bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic test`
  - `bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1 --config retention.ms=172800000 --topic test`
    - `--partitions` 옵션은 파티션 개수를 지정하며 최소 개수는 1개이다. 이 옵션을 지정하지 않으면 브로커 설정파일의 num.partitions 옵션값을 따른다.
    - `--replication-factor`는 파티션을 복제할 개수이다. 1은 복제하지 않고 사용한단 의미이다.
    - `--config`를 통해 추가적인 옵션을 설정할 수 있는데 retention.ms는 토픽의 데이터를 유지하는 기간으로 172800000ms는 2일을 의미한다. 즉 2일이 지난 토픽의 데이터는 삭제된다.
- 토픽 리스트 조회예시
  - `bin/kafka-topics.sh --bootstrap-server localhost:9092 --list test`
- 토픽 상세 조회
  - `bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test`
- 다양한 명령어들

  ```txt
    // 파티션 늘리기
    bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic test --alter --partitions 2

    // 리텐션 늘리기
    bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name test --alter --add-config retention.ms=86400000

    // 리텐션 확인
    bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name test --describe
  ```

### 2.2.2 kafka-console-producer.sh

- 토픽에 넣는 데이터는 레코드라고 부르며, 키-값으로 이루어져있다. 키가 없이 메시지를 보내면 아래와 같다.
  - `bin/kafka-console-producer.sh  --bootstrap-server localhost:9092 --topic test`
- 키를 추가한다면 명령어는 아래와 같다.
  - `bin/kafka-console-producer.sh  --bootstrap-server localhost:9092 --topic test --property "parse.key=true" --property "key.separator=:"`
  - 키 구분값을 명시적으로 : 으로 지정했고 없다면 기본설정은 탭(\t)이다.
- 키가 없으면(null) 라운드 로빈 방식으로 파티션에 적재되고 키가 있다면 동일한 파티션으로 전송된다.
- 만약 파티션 개수가 늘어나면 새로 프로듀싱되는 레코드들은 어느 파티션으로 갈까?
  - 키를 가진 메시지의 경우 파티션이 추가되면 파티션과 메시지 키의 일관성이 보장되질 않는다. 즉 이전에 키를 가진 메시지가 파티션 0번에 들어갔다면 파티션을 늘린 후 0번으로 간다는 보장이 없다. 파티션을 추가하더라도 일관성을 보장하고 싶다면 커스텀 파티셔너를 만들어야 한다.

### 2.2.3 kafka-console-consumer.sh

- 토픽으로 전송한 데이터는 kafka-console-consumer 명령어로 확인할 수 있고 --from-beginning 옵션을 주면 가장 처음 데이터부터 출력한다.
  - `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning`
- 만약 데이터의 키와 값을 확인하고 싶다면 --property 옵션을 사용한다.
  - `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --property print.key=true --property key.separator="-" --group test-group --from-beginning`
  - 위 명령어에서 group 옵션을 통해 컨슈머 그룹을 생성했다. 컨슈머 그룹은 1개 이상의 컨슈머로 이루어져 있다. 컨슈머 그룹을 통해 가져간 토픽의 메시지는 가져간 메시지에 대해 커밋을 한다. 커밋이란 컨슈머가 특정 레코드까지 처리를 완료했다고 레코드의 오프셋 번호를 브로커에 저장하는 것이다.
- kafka-console-consumer 명령어로 데이터를 가져가게 되면 토픽의 모든 파티션으로부터 동일한 중요도로 데이터를 가져간다. 이로 인해 프로슈서가 넣은 데이터의 순서와 컨슈머가 가져가는 데이터의 순서가 달라질 수 있다. 만약 토픽에 넣은 데이터의 순서를 보장하고 싶다면 가장 좋은 방법은 파티션 1개로 구성된 토픽을 만드는 것이다. 한 개의 파티션에서는 데이터의 순서를 보장하기 때문이다.

### 2.2.4 kafka-consumer-group.sh

- 컨슈머 그룹은 따로 생성 명령어를 날리지 않고 컨슈머가 동작할 때 그룹이름을 지정하면 새로 생성된다. 생성된 컨슈머 그룹의 리스트는 아래 명령어로 확인 가능하다.
  - `bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list`
- 컨슈머 그룹의 이름을 토대로 어떤 토픽의 데이터를 가져가는지 확인하려면 아래 명령어가 사용된다.
  - `bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --describe`
  - 컨슈머 그룹의 상세정보를 확인하는 것은 컨슈머 개발이나 카프카 운영할 때 중요하게 활용된다. 컨슈머 그룹이 중복되지 않았는지 컨슈머 랙이 있진 않은지 활용할 수 있다.

### 2.2.5 kafka-verifiable-producer, consumer.sh

- string 메시지를 주고 받으며, 카프카 클러스터 설치가 완료되고 간단한 네트워크 통신 테스트를 할 때 유용하다.
  - `bin/kafka-verifiable-producer.sh --bootstrap-server localhost:9092 --max-message 5 --topic verify-test`
  - `bin/kafka-verifiable-consumer.sh --bootstrap-server localhost:9092 --topic verify-test --group-id test-group`

### 2.2.6 kafka-delete-record.sh

- 이미 적재된 토픽의 데이터를 지우는 방법으로 kafka-delete-record.sh을 사용할 수 있다. 이미 적재된 데이터 중 가장 오래된 데이터부터 특정 시점의 오프셋까지 삭제할 수 있다. 예를 들어 0번 부터 10번까지 데이터를 지우고 싶다면 아래와 같이 입력한다.
  - delete.json 파일을 작성
    - `{"partitions": [{"topic":"test", "partition":0, "offset":3}], "version":1}]`
  - `bin/kafka-delete-records.sh --bootstrap-server localhost:9092 --offset-json-file delete.json`

## 3. 카프카 기본 개념 설명

### 3.1 카프카 브로커 / 클러스터 / 주키퍼

- 브로커는 데이터를 주고받기 위해 사용하는 주체이자, 데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션이다. 데이터를 안전하게 보관하고 처리하기 위해 3대 이상의 브로커 서버를 1개의 클러스터로 묶어 운영한다.
- 프로듀서가 보낸 데이터를 브로커가 토픽의 파티션에 데이터를 저장하고, 컨슈머 요청이 들어오면 데이터를 전달한다. 프로듀서가 전달한 데이터는 파일 시스템에 저장된다.
- 파일 시스템에 저장되기 때문에 파일 I/O로 속도가 느리지 않을까 걱정할 수 있다. 카프카는 페이지 캐시를 사용하여 디스크 I/O 속도를 높여 이 문제를 해결한다. 페이지 캐시란 OS에서 파일 입출력 성능 향상을 위해 만들어 놓은 메모리 영역으로 한 번 읽은 파일의 내용을 메모리의 페이지 캐시 영역에 저장하고, 추후 동일한 파일 접근이 일어나면 메모리에서 직접 읽는 방식이다.
- 데이터 복제는 카프카를 장애허용 시스템으로 동작하게 만든다. 데이터 복제는 파티션 단위로 이루어지며 토픽을 생성할 때 파티션 복제 개수(replication factor)를 설정할 수 있고 기본 값은 브로커에 설정된 옵션을 따라간다.
- 프로듀서 / 컨슈머와 직접 통신하는 파티션을 리더 파티션이라고 부르고 나머지 복제본은 팔로워 파티션이라 부른다. 팔로워 파티션들은 리더 파티션의 오프셋을 확인해 자신의 오프셋과 차이가 나는 경우 리더 파티션의 데이터를 가져와 자신의 파티션에 저장하는데 이를 복제라고 부른다.
- 3개의 브로커로 이루어진 클러스트에서 리더 파티션에 장애가 발생해도 팔로워 파티션 중 하나가 리더 지위를 넘겨받아 데이터 유실 없이 프로듀서 / 컨슈머와 동작한다. 데이터가 일부 유실되어도 무관하고 속도가 중요하다면 복제 개수를 1 or 2로 설정한다. 금융 정보와 같이 유실되어선 안되는 데이터의 경우 복제 개수를 3으로 설정해 데이터를 안정적으로 유지해야 한다.
- 다수 브로커 중 한대가 컨트롤러의 역할을 한다. 컨트롤러는 다른 브로커들의 상태를 체크하고 브로커에 장애가 나거나 빠지는 경우 해당 브로커에 존재하는 리더 파티션을 재분배한다. 만약 컨트롤러 역할을 하는 브로커에 장애가 발생하면 다른 브로커가 컨트롤러 역할을 한다.
- 주키퍼는 카프카의 메타 데이터(host, port, 어떤 브로커가 컨트롤러인지, 저장된 토픽)를 관리하는데 사용된다.

### 3.2 토픽과 파티션

- 토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위다. 토픽은 1개 이상의 파티션을 소유하며 파티션에 저장된 데이터를 레코드라 부른다.
- 컨슈머 처리량이 한정된 상황에서 처리량을 증가하려면 가장 좋은 방법은 컨슈머의 개수를 늘림과 동시에 파티션 개수도 늘리는 것이다.
- 파티션은 큐와 비슷한 구조로 먼저 들어간 레코드는 컨슈머가 먼저 가져가게 되며, 데이터를 가져가면 카프카는 데이터를 삭제하지 않는다.
- 토픽 이름 조건
  - 토픽 이름은 케밥 케이스 (-)나 스네이크 케이스(_)를 소문자로 사용하는게 좋으며 이름 예시를 들면 아래와 같다.
  - [환경].[팀명].[애플리케이션명].[메시지타입]
    - prod.marketing-team.sms-platform.json
  - [프로젝트명].[서비스명].[환경].[이벤트명]
    - community.payment.prod.notification
  - [환경].[서비스명].[JIRA번호].[메시지타입]
    - dev.eamil-sender.jira-1234.email-vo-custom
  - 중요한 것은 토픽이름에 대한 규칙을 정하고 따르는 것으로, 카프카는 토픽이름 변경을 지원하지 않으므로 삭제 후 다시 생성하는 것 밖에 방법이 없다.

### 3.3 레코드

- 프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장된다. 브로커에 한 번 적재된 레코드는 수정될 수 없고 리텐션 기간 or 용량에 따라서만 삭제된다.

### 3.4 카프카 클라이언트

- 카프카 클라이언트 라이브러리는 카프카 클러스터에 명령을 내리거나 데이터를 송/수신하기 위해 사용하며 카프카 클라이언트는 프레임워크나 애플리케이션 위에서 구현하고 실행해야 한다.

#### 3.4.1 프로듀서 API

- 코드단에서 kafkaProducer 인스턴스가 send 메서드를 호출하면 파티셔너에서 토픽의 어떤 파티션으로 전송될 것인지 정해진다. 파티셔너에 의해 구분된 레코드는 데이터를 전송하기 전에 버퍼에 쌓아놓고 전송함으로써 프로듀서의 처리량 향상에 도움을 준다.
- 카프카 클라이언트 라이브러리 2.5.0 버전에서 파티셔너를 지정하지 않으면 UniformStickyPartitioner 가 기본 설정된다.
RoundRobinPartitioner 와의 차이는 배치에 묶이는 데이터의 양이다. UniformStickyPartitioner는 어큐뮬레이터에서 데이터가 배치로 모두 묶일때까지 기다렸다가 배치에 묶인 데이터를 동일한 파티션에 전송해 향상된 성능을 가지게 되었다.
- 프로듀서가 데이터를 보낼 때 응답을 동기로 기다릴지 비동기로 받을지 결정할 수 있는데, 만약 데이터 순서가 중요하다면 동기로 전송 결과를 응답받아야 한다. 비동기로 보낼 경우, 비동기로 A 다음 B를 보냈다고 가정하자. A가 실패하고 B가 성공하면 데이터 순서가 바뀔 수 있다.

#### 3.4.2 컨슈머 API

- 토픽의 파티션으로부터 데이터를 가져가기 위해 컨슈머를 운영하는 방법은 크게 2가지가 있다.
  - 1개 이상의 컨슈머로 이루어진 컨슈머 그룹을 운영하거나
  - 토픽의 특정 파티션만 구독하는 컨슈머를 운영하는 것이다.
- 컨슈머 그룹으로 운영한다면 파티션과 컨슈머의 관계를 1대 다로 봐도 된다. 한 파티션은 하나의 컨슈머랑 매핑되며 반대로 하나의 컨슈머를 여러 파티션에 할당될 수 있다. 이런 특징으로 컨슈머 그룹의 컨슈머 개수는 토픽의 파티션 개수와 같거나 작아야 한다.
- 만약 3개의 파티션을 가진 토픽이 있다면 3개 이하의 컨슈머로 구성된 컨슈머 그룹으로 운영해야 한다. 만약 4개의 컨슈머가 있다면 1개의 컨슈머를 놀게된다.
- 컨슈머 그룹으로 운영할 때의 장점은 다른 컨슈머 그룹과 격리되어 영향을 받지 않는다. 예를 들어 리소스 정보가 토픽에 저장된다면 이를 소비하는 ES 컨슈머 그룹과 하둡 컨슈머 그룹이 있을 때 ES에 장애가 발생해도 하둡 컨슈머 그룹은 영향을 받지 않는다.
- 컨슈머 그룹의 컨슈머에 장애가 발생하면, 장애가 발생한 컨슈머에 할당된 파티션의 소유권이 정상 컨슈머에게 넘어간다. 이 과정을 리밸런싱이라고 부르며 컨슈머가 추가되거나 제외될 때 일어난다.
- 컨슈머는 브로커로부터 데이터를 어디까지 가져갔는지 커밋을 통해 기록한다. 특정 토픽의 파티션을 어떤 컨슈머 그룹이 몇 번째까지 가져갔는지 브로커 내부의 내부 토픽(_consumer_offsets)에 기록된다. 만약 컨슈머 동작 문제로 오프셋 커밋이 실패했다면 데이터 처리의 중복이 발생할 수 있다.
- 오프셋 커밋은 명시적, 비명시적으로 수행할 수 있고, 기본옵션은 poll 메소드가 수행될 때 일정 간격마다 오프셋을 커밋하도록 설정되어 있다. 편리한 방식이지만 poll 메서드 호출 이후에 리밸런싱 or 컨슈머 강제 종료시 데이터 중복 or 유실의 문제가 있다. 그러므로 데이터 중복이나 유실을 허용하지 않는다면 명시적으로 오프셋을 커밋해야 한다.
- 명시적으로 오프셋을 커밋하려면 poll 메서드 호출된 후 반환받은 데이터의 처리가 완료되고 commitSync을 호출하면 된다. commitSync은 브로커에 커밋 요청을 하고 응답을 기다리므로 컨슈머 처리량에 영향을 미치므로 운영 목적에 맞게 설정해야 한다.
- 컨슈머 그룹에서 컨슈머가 추가/삭제되면 파티션을 컨슈머에 재할당하는 리밸런스가 일어난다. poll 메서드를 통해 반환받은 데이터를 모두 처리하기 전에 리밸런스가 발생하면 데이터를 중복처리할 수 있다. Poll 메서드를 통해 받은 데이터 일부를 처리했으나 커밋하지 않았기 때문이다. 리밸런스 발생 시 데이터를 중복처리하지 않기 위해서는 처리한 데이터를 기준으로 커밋을 시도해야 한다.

#### 3.4.3 어드민 API

- 실제 운영환경에서는 카프카에 설정된 내부 옵션을 설정하고 확인하는 것이 중요하다. 내부 옵션을 확인하는 확실한 방법은 브로커 중 한대에 접속해 브로커 옵션을 확인하는 것이지만 매우 번거로운 작업이다.
- 카프카 클라이언트에서는 내부 옵션을 설정하거나 조회하기 위해 AdminClient 클래스를 제공한다. AdminClient를 활용해 ACL 접근 규칙을 추가하거나 특정 토픽의 데이터 양이 늘어남을 감지하고 토픽의 파티션을 늘리는 등의 작업을 할 수 있다.

### 3.5 카프카 스트림즈

- 카프카 스트림즈는 토픽에 적재된 데이터를 상태기반 or 비상태기반으로 실시간 변환하여 다른 토픽에 적재하는 라이브러리다.
카프카를 운영하면서 실시간 스트림 처리를 해야한다면 카프카 스트림즈 애플리케이션으로 개발하는 것을 1순위로 고려하는게 좋다.
- 앞에서 설명한 프로듀서 / 컨슈머 개념으로 스트림즈가 제공하는 기능을 유사하게 만들 수 있다. 그러나 스트림즈 라이브러리를 통해 제공하는 단 한번의 데이터 처리, 장애 허용 시스템 특징은 프로듀서 / 컨슈머 조합으로 완벽하게 구현하기는 어렵다.
- 카프카 스트림즈에서는 토폴로지의 노드를 프로세서라 부르고 노드와 노드를 이은 선을 스트림이라 부른다. 스트림은 토픽의 데이터를 뜻하고 프로세서에는 소스, 스트림, 싱크 프로세서 3가지가 있다. 소스 프로세서는 하나 이상의 토픽에서 데이터를 가져오고, 스트림 프로세서는 다른 프로세서가 반환한 데이터를 변환, 분기처리를 담당한다. 싱크 프로세서는 변환된 데이터를 특정 토픽으로 저장하는 최종 종착지다.

#### 3.5.1 스트림즈 DSL

- 스트림즈 DSL에는 레코드의 흐름을 추상화한 3가지 개념인 KStream, KTable, GlobalKTable이 있다.
- KStream은 레코드의 흐름을 표현한 것으로 메시지 키와 값으로 구성되어 있다. KStream을 조회하면 토픽에 존재하는 모든 레코드가 출력된다.
- KTable은 메시지 키를 기준으로 묶어서 사용하며 KStream은 토픽의 모든 레코드를 조회할 수 있지만 KTable은 유니크한 키를 기준으로 가장 최신 레코드를 사용한다. 따라서 새로 데이터를 적재할 때 동일한 키가 있다면 데이터가 업데이트 되었다고 볼 수 있다.
- GlobalKTable은 KTable과 동일하게 키를 기준으로 묶어서 사용한다. 그러나 KTable로 선언된 토픽은 1개 파티션이 1개 태스크에 할당되어 사용되고 GlobalKTable로 선언된 토픽은 모든 파티션 데이터가 각 태스크에 할당되어 사용된다는 차이가 있다.

### 3.6 카프카 커넥터

- 데이터 파이프라인 생성시 반복 작업을 줄이고 효율적인 전송을 위한 애플리케이션이다. 프로듀서 / 컨슈머로 파이프라인을 만드는 것도 좋은 방법이지만 반복적인 파이프라인 생성 작업이 있을 때는 매번 프로듀서, 컨슈머 애플리케이션을 개발하고 배포 운영해야 하기 때문에 비효율적이다.
- 카프카 커넥터는 프로듀서의 역할을 하는 소스 커넥터와 컨슈머 역할을 하는 싱크 커넥터로 나뉜다. 소스 커넥터는 소스 애플리케이션 or 소스 파일로부터 데이터를 가져와 토픽으로 넣는 역할을 한다. 싱크 커넥터는 토픽의 데이터를 타겟 어플리케이션 or 타겟 파일로 저장하는 역할을 한다.

### 3.7 카프카 미러메이커2

- 카프카 미러메이커2는 서로 다른 카프카 클러스터 간에 토픽을 복제하는 어플리케이션이다. 굳이 미러메이커2를 사용하는 이유는 토픽의 모든 것을 복제할 필요성이 있기 때문이다. 토픽에 있는 레코드는 고유한 키, 값, 파티션을 가지는데 프로듀서 / 컨슈머를 사용해서 완전히 동일하게 복사하는 것은 어려운 일이다.
- 미러메이커2를 사용하여 카프카 클러스터 단위의 활용도를 높일 수 있다. 미러메이커2가 제공하는 단방향, 양방향 복제 기능, ACL 복제, 새 토픽 자동감지 기능은 클러스터가 2개 이상 있을 때 더욱 빛난다.

#### 액티브 스탠바이 클러스터 운영

- 어플리케이션과 통신하는 카프카 클러스터 외에 재해 복구를 위해 임시 카프카 클러스터를 하나 더 구성하는 경우 액티브-스탠바이 클러스터로 운영할 수 있다. 이때 미러메이커2를 사용해 액티브 클러스트의 모든 토픽을 스탠바이 클러스터에 복재해 장애에 대응할 수 있다.
- 클러스터에 재해가 생기는 경우는 자연재해, 기술적 재해(EMP 공격, 데이터 센터 중단), 인간에 의한 재해이다. 이러한 재해에 대응하기 가장 좋은 방법은 물리적인 공간을 분리하는 것이다. 예로 한국에서 서비스를 하고 있다면 액티브 클러스터는 한국의 데이터 센터에, 스탠바이 클러스터는 일본의 데이터 센터에 운영할 수 있다.

#### 액티브 액티브 클러스터 운영

- 글로벌 서비스를 운영할 경우 서비스 어플리케이션 지연을 최소화하기 위해 2개 이상의 클러스터를 두고 데이터를 미러링하면서 사용할 수 있늠데 이 때 액티브-액티브 클러스터를 운영하는게 하나의 방법이 될 수 있다.

#### 허브 앤 스포크 클러스터 운영

- 각 팀에서 소규모 클러스터를 사용하고, 각 팀의 데이터를 한 개의 클러스터에 모아 데이터 레이크로 사용하고 싶다면 허브 앤 스포크 구성이 한 가지 방안이 될 수 있다. 미러메이커2를 사용해 각 팀에서 사용하는 카프카 클러스터에 존재하는 데이터를 수집하고 데이터 레이크 역할을 하는 클러스터에서 가공, 분석하여 가치있는 데이텅를 찾아낼 수 있다.

## 4. 카프카 상세 개념 설명

- 상세 개념을 모르고 기본 설정값만 가지고 운영한다면 카프카를 반쪽만 활용하는 것이다.

### 4.1 토픽과 파티션

- 토픽은 카프카의 시작과 끝이다. 운영상에서 토픽에 대한 고려사항을 알아보자.

#### 4.1.1 적정 파티션 개수

- 토픽 최초 생성 시, 파티션 개수를 정할 떄 고려할 점은 3가지가 있다.
  - 데이터 처리량
    - 데이터 처리 속도를 올리는 확실한 방법은 파티션 개수를 늘리고 파티션 개수만큼 컨슈머를 추가하는 방법이다. 프로듀서가 보내는 데이터양과 컨슈머의 데이터 처리량을 계산해 파티션 개수를 정해야 한다. 만약 프로슈더가 초당 1,000 레코드를 보내고 컨슈머가 처리할 수 있는 데이터가 초당 100 레코드라면 필요한 파티션 개수는 10개이다.
      - 파티션 개수 공식: 프로듀서 데이터 전송량 < 컨슈머 데이터 처리량 * 파티션 개수
    - 컨슈머 데이터 처리량이 프로듀서가 보내는 데이터보다 작다면 컨슈머 랙이 생기고 데이터 처리 지연이 발생하게 된다.
    - 컨슈머 데이터 처리량을 구하는 방법은 상용에서 운영중인 카프카에서 더미 데이터로 테스트를 해보는 것이다. 컨슈머 데이터 처리량을 구한 뒤에는 프로듀서가 보내는 데이터양을 하루, 시간, 분 단위로 쪼개서 예측한다. 만약 지연이 절대 발생해선 안 된다면 프로듀서가 보내는 데이터의 최대치를 잡고 계산하면 된다.
  - 메시지 키 사용 여부
    - 메시지 키를 사용하면 프로듀서가 토픽으로 데이터를 보낼 때 메시지 키를 해시 변환하여 파티션에 매칭시킨다. 만약 파티션 개수가 달라지면 다른 파티션에 데이터가 할당된다. 따라서 파티션 개수가 달라지는 순간 메시지 키를 사용하는 컨슈머는 특정 메시지 키의 순서를 더는 보장받지 못한다.
    - 메시지 키를 사용하고 컨슈머에서 처리 순서가 보장되어야 한다면 최대한 파티션 변화가 발생하지 않는 방향으로 운영해야 한다. 만약 파티션 개수가 변한다면 메시지 키의 매칭을 그대로 가져가기 위해 커스텀 파티셔너를 개발하고 적용해야 한다. 따라서 메시지 키별로 처리 순서를 보장하기 위해 파티션 개수를 넉넉합게 잡고 생성하는 것이 좋다.
    - 반대로 처리 순서를 지키지 않아도 된다면 처음부터 넉넉하게 잡지 않아도 된다. 데이터 양에 따라 파티션을 늘리면 되기 때문이다.

#### 4.1.2 토픽 정리 정책 (cleanup.policy)

- 토픽의 데이터는 시간 or 용량에 따라 삭제 규칙을 적용할 수 있다. 또한 삭제를 원치 않으면 토픽의 데이터를 삭제하지 않도록 설정할 수도 있다. 그러나 데이터를 오랫동안 삭제하지 않으면 저장소 사용량이 지속적으로 늘어나 AWS MSK를 사용한다면 저장소 용량이 늘어남에 따라 카프카 운영 비용도 함께 늘어날 수 있다.
- cleanup.policy는 2가지 삭제 정책을 제공하는데 완전 삭제와 압축으로 동일 메시지 키의 가장 오랜 데이터를 삭제하는 방법이다.
- 토픽 삭제 정책
  - 세그먼트 단위로 삭제를 진행한다. 세그먼트는 토픽의 데이터를 저장하는 명시적인 파일시스템 단위다.
  - 세그먼트는 파티션마다 별개로 생성되며, 시간 or 용량을 기준으로 삭제된다. 삭제된 데이터는 복구할 수 없다.
    - 세그먼트 파일의 마지막 수정시간이 retention.ms를 넘어가면 세그먼트는 삭제된다.
    - retention.bytes를 넘어간 세그먼트 파일들은 삭제된다.
- 토픽 압축 정책
  - 압축은 일반적으로 생각하는 zip과는 다른 개념이다. 여기서 압축이란 메시지 키별로 오래된 데이터를 삭제하는 정책을 뜻한다.
  - 메시지 키를 기준으로 오래된 데이터를 삭제하기 때문에 오프셋의 증가가 일정하지 않을 수 있다.

#### 4.1.3 ISR(In-Sync-Replicas)

- ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻한다. 이 용어가 나온 이유는 팔로워 파티션이 리더 파티션으로부터 데이터를 복제하는데 시간이 걸리기 때문이다. 리더 파티션에 데이터가 추가된 후 팔로워 파티션이 복제하는 시간차 때문에 리더와 팔로워 사이에 오프셋 차이가 발생한다.
- 이를 모니터링 하기 위해 리더 파티션을 replica.lag.time.max.ms 주기를 가지고 팔로워 파티션이 데이터를 복제하는지 확인한다. 만약 설정값보다 긴 시간동안 데이터를 가져가지 않으면 팔로워 파티션에 문제가 생긴것으로 판단하고 ISR 그룹에서 제외한다.
- ISR로 묶인 리더와 파티션에 존재하는 데이터가 모두 동일하기 때문에 팔로워는 리더 파티션으로 새로 선출될 자격을 가진다. 반대로 ISR로 묶이지 않으면 리더로 선출될 자격이 없다. 왜냐면 데이터가 복제되지 못한 상태이고 이 상태에서 팔로워가 리더로 선출되면 데이터가 유실될 수 있기 때문이다.
- 데이터의 유실을 감수하더라도 서비스를 중단하지 않고 토픽을 사용하고 싶다면 unclean.leader.election을 true로 주면 ISR로 묶이지 않은 팔로워도 리더 파티션으로 선출될 수 있다.

### 4.2 카프카 프로듀서

- 프로듀서의 acks 옵션은 0, 1, all 값을 가질 수 있다.

#### acks = 0

- 프로듀서가 리더로 데이터 전송 시, 리더 파티션으로 데이터가 저장되었는지 확인하지 않는다는 뜻이다. 프로듀서는 리더 파티션으로부터 응답을 받지 않고 지속적으로 다음 데이터를 보내기 때문에 빠르며, 데이터가 일부 유실되더라도 전송 속도가 중요한 경우 이 옵션을 사용할 수 있다.

#### acks = 1

- 리더 파티션에만 정상적으로 데이터가 저장되었는지 확인한다. 리더 파티션에 데이터가 저장되도 데이터 유실이 발생할 수 있는데 리더가 데이터를 저장하고 팔로워가 복제하기 전에 리더 파티션이 있는 브로커에 장애가 발생하는 경우다.

#### acks = all or -1

- 리더와 팔로워 파티션에 모두 정상적으로 저장되었는지 확인한다. 데이터를 안전하게 전송하고 저장할 수 있지만 느리다는 단점이 있다.
- 옵션 값이 all 일 경우 min.insync.replicas 옵션값에 따라 몇 개까지 복제를 보장할지 달라진다. 운영하는 브로커 개수가 이 옵션값보다 작으면 프로듀서는 더 이상 데이터를 전송할 수 없다. 이 옵션은 절대로 브로커 개수와 동일한 숫자로 설정하면 안 된다. 클러스터의 버전 업그레이드가 발생하면 브로커는 롤링 다운 타임이 발생하는데 이ㄷ 때 프로듀서가 데이터를 추가할 수가 없다.

#### 4.2.2 멱등성 프로듀서

- 멱등성 프로듀서는 동일한 데이터를 여러 번 전송해도 클러스터에는 단 한번만 저장됨을 의미한다. 프로듀서의 기본 동작은 적어도 한 번 전달(at least once delivery)을 지원한다. 적어도 한 번 전달이란 적어도 한 번 이상 데이터를 적재할 수 있고 데이터가 유실되지 않음을 뜻 하지만 데이터의 중복이 발생할 수 있다.
- 데이터 중복 적재를 막기위해 0.11.0 이후 버전부터는 프로듀서에서 enable.idempotence 옵션을 사용해 정확히 한번 전달을 지원한다. 멱등성 프로듀서는 브로커로 데이터를 전달할 때 프로듀서 PID와 시퀀스 넘버를 함께 전달한다. 브로커는 프로듀서의 PID와 시퀀스 넘버를 확인하여 동일한 메시지의 적재 요청이 오더라도 단 한번만 데이터를 적재하도록 동작한다.
- 멱등성 프로듀서는 동일한 세션에서만 정확히 한 번 전달을 보장한다. 만약 멱등성 프로듀서로 동작하는 어플리케이션이 종료되고 재시작되면 PID가 달라진다. PID가 달라지면 브로커 입장에선 다른 프로듀서 어플리케이션에서 데이터를 보냈다고 판단하기 때문에 장애가 발생하지 않을 경우에만 정확히 한 번 적재하는 것을 고려해야 한다.
- 멱등성 프로듀서의 시퀀스 넘버는 0부터 시작해서 1씩 증가된 값이 전달된다. 브로커는 전달받은 데이터의 PID와 시퀀스를 확인하는 과정에서 시퀀스 번호가 일정하지 않은 경우에는 예외가 발생할 수 있다. 예외가 발생하면 시퀀스 넘버의 순서가 맞지 않음을 의미하므로 순서가 중요한 데이터를 전송하는 프로듀서는 예외 발생시 대응하는 방안을 고려해야 한다.

#### 4.2.3 트랜잭션 프로듀서

- 파티션에 데이터를 저장할 경우 모든 데이터에 대해 동일한 원자성을 제공하기 위해 사용한다. 다수의 데이터를 동일한 트랜잭션으로 묶음으로써 전체 데이터를 처리하거나 처리하지 않도록 한다.
- 트랜잭션 프로듀서는 데이터를 파티션에 저장할 때 트랜잭션의 시작과 끝을 표현하는 레코드를 한 개 더 보낸다. 트랜잭션 컨슈머는 파티션에 저장된 트랜잭션 레코드를 보고 트랜잭션이 완료되었음을 확인하고 데이터를 가져간다.

### 4.3 카프카 컨슈머

#### 4.3.1 멀티 스레드 컨슈머

- 멀티 스레드로 컨슈머를 안전하게 운영하기 위해서는 비정상 종료나 예외 상황등 고려할 부분이 많지만 안정적으로 운영할 수 있다면 매우 효율적으로 컨슈머를 운영할 수 있다.

- 카프카 컨슈머 멀티 워커 스레드 전략
  - 컨슈머 스레드는 1개만 실행하고 데이터 처리를 담당하는 워커 스레드를 여러개 실행하는 방법이다.
  - 브로커로 전달받은 레코드들을 병렬로 처리한다면 1개의 컨슈머 스레드로 받은 데이터들을 더욱 향상된 속도로 처리할 수 있다.
  - 레코드 처리에 중복이 발생하거나 데이터의 역전현상이 발생해도 되며 매우 빠른 처리속도가 필요한 데이터 처리에 적합하다.
- 카프카 컨슈머 멀티 스레드 전략
  - 하나의 파티션은 컨슈머 1개가 할당되고 하나의 컨슈머는 여러 파티션에 할당될 수 있다. 이 특징을 살리기 위해 1개의 어플리케이션에 구독하고자 하는 토픽의 파티션 개수만큼 컨슈머 스레드 개수를 늘려서 운영하는 방법이다.

#### 4.3.2 컨슈머 랙

- 컨슈머 랙은 토픽의 최신 오프셋과 컨슈머 오프셋간의 차이다. 컨슈머가 정상 동작하는지 확인할 수 있기 때문에 컨슈머 어플리케이션을 운영한다면 필수적으로 모니터링해야 하는 지표이다.
- 내비게이션 사용자 데이터를 전송하는 프로듀서가 있다고 가정하자. 추석이나 설에는 프로듀서가 전송하는 데이터 양이 확 올라간다. 컨슈머는 처리량이 한정되어 있어 컨슈머 랙이 발생할 수 있다. 이 경우 지연을 줄이기 위해 일시적으로 파티션 개수와 컨슈머 개수를 늘려서 병렬처리량을 늘리는 방법을 사용할 수 있다.
- 컨슈머 랙을 확인하는 방법은 명령어를 사용하는 방법, 컨슈머 어플리케이션에서 metrics() 메서드를 사용하는 방법, 외부 모니터링 툴을 이용하는 방법이 있다.
  - 카프카 명령어
    - kafak-consumer-group.sh 명령어를 사용하면 컨슈머 랙을 포함한 컨슈머 그룹의 상태를 확인할 수 있다.
  - 외부 모니터링 툴
    - 컨슈머 랙을 모니터링 하는 최선의 방법이다. 데이터 독, 컨플루언트 컨트롤 센터 등 다양한 지표를 모니터링 할 수 있다.

#### 4.3.3 컨슈머 배포 프로세스

- 크게 중단배포와 무중단 배포로 나뉜다.
- 중단 배포
  - 컨슈머 어플리케이션을 완전히 종류한 이후, 개선된 코드를 가진 어플리케이션을 배포하는 방식이다. 물리적인 서버를 운영하는 기업에 적합하다.
- 무중단 배포
  - 인스턴스 발급과 반환이 유연한 가상 서버를 사용하는 경우에 유용하다. 무중단 배포 방법은 블루/그린, 롤링, 카나리가 있다.
  - 블루/그린 배포는 이전 버전 어플리케이션과 신규 어플리케이션을 동시에 띄어놓고 트래픽을 전환하는 방법이다. 파티션의 개수와 컨슈머 개수를 동일하게 실행하는 어플리케이션을 운영할 때 유용하다.
  - 롤링 배포는 인스턴스의 할당과 반환으로 리소스를 줄이면서 무중단 배포를 할 수 있다.
  - 카나리 배포는 소수 파티션에 신규 버전 어플리케이션을 할당하여 사전 테스트를 진행할 수 있다.

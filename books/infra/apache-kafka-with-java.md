# 아파치 카프카 애플리케이션 프로그래밍 with 자바

- [책 링크](https://www.yes24.com/Product/Goods/99122569)

## 1. 들어가며

### 1.1 카프카의 탄생

- 링크드인은 초기에 단방향 통신을 통해 소스 어플리케이션에서 타겟 어플리케이션으로 연동하는 소스 코드를 작성하여 운영했다. 시간이 지날수록 아키텍처는 복잡해졌고 소스/타겟 어플리케이션이 많아지면서 데이터를 전송하는 라인이 복잡해지기 시작했다.
- 링크드인의 데이터 팀은 신규 시스템을 만들기로 결정했고 그 결과물이 아파치 카프카다. 각 어플리케이션이 연결하여 데이터를 처리하는 것이 아니라 한 곳에 모아 처리할 수 있도록 중앙집중화했다. 웹 사이트, 어플리케이션, 센서에서 취합한 데이터 스트림을 카프카 한 곳에서 수집하고 사용자들이 실시간으로 소비할 수 있게 동작한다.
- 기존에 1:1 매칭으로 운영하던 시스템에선 한쪽의 장애가 한쪽에 영향을 미치곤 했지만 카프카는 이러한 의존도를 타파한다. 소스 어플리케이션에서 생성되는 데이터는 어떤 타겟 어플리케이션으로 보낼 것인지 고민하지 않고 카프카로 넣으면 된다. 카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 방식의 큐와 유사하다. 큐에 데이터를 보내는 것이 프로듀서고, 큐에서 데이터를 가져가는 것이 컨슈머다.
- 상용 환경에서 카프카는 3대 이상의 브로커에서 분산 운영하여 데이터를 안전하게 기록한다. 3대 이상으로 이루어진 카프카 클러스터 중 일부 서버에 장애가 발생하더라도 데이터를 지속적으로 복제하기 때문에 안전하게 운영할 수 있다.

### 1.2 빅데이터 파이프라인에서 카프카 역할

- 빅 데이터를 저장하고 활용하기 위해, 우선 생성되는 데이터를 모두 모으는 것이 중요한데 이때 사용되는 개념이 데이터 레이크다. 데이터 웨어 하우스와는 다르게 필터링되거나 패키지화 되지 않은 데이터가 저장된다는 점이 특징이다.
- 서비스에서 발생하는 데이터를 데이터 레이크에 모으려면 단순히 생각할 때 발생하는 데이터를 직접 end-to-end 방식으로 넣을 수 있다. 하지만 서비스가 커지고 복잡해질수록 링크드인처럼 파편화되고 복잡도가 올라가는 문제가 발생한다. 이 때 데이터 파이프라인을 안정적이고 확장성 높게 운영하기 위해 좋은 방법 중 하나가 카프카를 활용하는 것이다. 왜 카프카가 적합한지 상세히 살펴보자.
  - 높은 처리량: 카프카는 프로듀서/컨슈머가 데이터를 보내고 받을 때 모두 묶어서 전송한다. 많은 양의 데이터를 묶음 단위로 배치에서 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그데이터를 처리하는데 적합하다.
  - 확장성: 카프카는 데이터 양이 가변적인 환경에서 안정적으로 확장 가능하도록 설계되었다. 데이터가 적을 때는 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면 브로커 개수를 늘려 스케일 아웃 할 수 있다.
  - 영속성: 영속성은 프로그램이 종료되더라도 사라지지 않는 데이터 특성을 뜻한다. 카프카는 다른 메시징 시스템과는 다르게 전송받은 데이터를 파일 시스템에 저장한다. 파일 시스템을 사용하는 것이 느리다고 생각하겠지만 카프카는 OS 레벨에서 파일 I/O 성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성해서 사용한다.
  - 고가용성: 3개 이상의 서버로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다.
- 카프카 클러스터를 3대 이상의 브로커들로 구성해야 하는 이유
  - 브로커를 1대만 유지한다면 테스트 목적으로만 사용한다.
  - 브로커를 2대로 유지하면 한 대의 브로커에 장애가 발생하더라도 한 대가 살아 있어 안정적으로 데이터를 처리할 수 있다. 하지만 브로커 간에 데이터가 복제되는 시간차이로 일부 데이터가 유실될 가능성이 있다. 예를 들면 leader가 데이터를 받고 follower가 복제를 해야 하는데 leader가 죽은 경우.
  - 유실을 막기 위해 min.insync.replicas 옵션을 사용할 수 있는데 2로 설정하면 최소 2개 이상의 브로커에 데이터가 완전히 복제됨을 보장한다. 이 옵션을 2로 설정하면 브로커를 3대 이상으로 운영해야 한다.
  - 정리하면 유실없이 데이터를 복제하기 위해 min.insync.replicas 값을 2로 설정해야 한다. 그래야 최소 2개 이상의 브로커에 데이터가 완전히 복제되니까. 근데 브로커가 2대이면 한 대가 죽을 경우 replicas 2보다 작은 한 대의 브로커만 받기 때문에 에러가 발생한다. 따라서 3대로 운영하고 min.insync.replicas 값이 2면 카프카 한 대가 죽어도 정상적으로 동작한다.

### 1.3 데이터 레이크 아키텍처와 카프카 미래

- 데이터 레이크를 구성하는 아키텍처는 크게 람다 아키텍처와 카파 아키텍처가 있다.
- 람다 아키텍처는 기존 e2e로 데이터를 수집하는 레거시를 개선하기 위해 구성된 아키텍처다. 배치 데이터를 일괄 처리하는 배치 레이어, 가공된 데이터를 제공하는 서빙 레이어, 실시간으로 데이터를 분석하는 스피드 레이어로 구성된다.
- 람다 아키텍처는 데이터를 배치 처리하는 레이어와 실시간 처리를 분담할 수 있었지만, 레이어가 2개로 나뉘기 때문에 생기는 단점들도 있었다. 제이 크랩스는 람다 아키텍처에서 배치 레이어를 제거한 카파 아키텍처를 제안했다.
- 카파 아키텍처에서는 스피드 레이어에서 데이터를 모두 처리할 수 있어 효율적으로 개발과 운영을 할 수 있었으나 서비스에서 생성되는 모든 종류의 데이터를 스트림 처리해야 했고, 서비스에서 생성된 모든 데이터가 스피드 레이어에 들어오는 것을 감안하면 내결함성과 장애 허용 특징을 지녀야 했다. 아파치 카프카는 이런 특성에 정확히 부합하는 플랫폼이다.

## 2. 카프카 빠르게 시작해보기

### 2.2.1 kafka-topic.sh

- 카프카 클러스터에는 토픽이 여러개 존재할 수 있다. 토픽에는 파티션이 존재하는데 파티션의 개수는 최소 1개부터 시작한다. 토픽을 생성하는 방법은 2가지가 있는데 아래와 같다.
  - 컨슈머, 프로듀서가 생성되지 않은 토픽에 데이터를 요청할 때
  - 커맨드라인 툴로 명시적으로 토픽을 생성할 때
- 토픽을 생성할 때는 명시적으로 생성하는 걸 추천한다. 토픽마다 처리되어야 하는 데이터 특성이 다르기 때문이다.
- 토픽 생성 예시
  - `bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic test`
  - `bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1 --config retention.ms=172800000 --topic test`
    - `--partitions` 옵션은 파티션 개수를 지정하며 최소 개수는 1개이다. 이 옵션을 지정하지 않으면 브로커 설정파일의 num.partitions 옵션값을 따른다.
    - `--replication-factor`는 파티션을 복제할 개수이다. 1은 복제하지 않고 사용한단 의미이다.
    - `--config`를 통해 추가적인 옵션을 설정할 수 있는데 retention.ms는 토픽의 데이터를 유지하는 기간으로 172800000ms는 2일을 의미한다. 즉 2일이 지난 토픽의 데이터는 삭제된다.
- 토픽 리스트 조회예시
  - `bin/kafka-topics.sh --bootstrap-server localhost:9092 --list test`
- 토픽 상세 조회
  - `bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test`
- 다양한 명령어들

  ```txt
    // 파티션 늘리기
    bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic test --alter --partitions 2

    // 리텐션 늘리기
    bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name test --alter --add-config retention.ms=86400000

    // 리텐션 확인
    bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name test --describe
  ```

### 2.2.2 kafka-console-producer.sh

- 토픽에 넣는 데이터는 레코드라고 부르며, 키-값으로 이루어져있다. 키가 없이 메시지를 보내면 아래와 같다.
  - `bin/kafka-console-producer.sh  --bootstrap-server localhost:9092 --topic test`
- 키를 추가한다면 명령어는 아래와 같다.
  - `bin/kafka-console-producer.sh  --bootstrap-server localhost:9092 --topic test --property "parse.key=true" --property "key.separator=:"`
  - 키 구분값을 명시적으로 : 으로 지정했고 없다면 기본설정은 탭(\t)이다.
- 키가 없으면(null) 라운드 로빈 방식으로 파티션에 적재되고 키가 있다면 동일한 파티션으로 전송된다.
- 만약 파티션 개수가 늘어나면 새로 프로듀싱되는 레코드들은 어느 파티션으로 갈까?
  - 키를 가진 메시지의 경우 파티션이 추가되면 파티션과 메시지 키의 일관성이 보장되질 않는다. 즉 이전에 키를 가진 메시지가 파티션 0번에 들어갔다면 파티션을 늘린 후 0번으로 간다는 보장이 없다. 파티션을 추가하더라도 일관성을 보장하고 싶다면 커스텀 파티셔너를 만들어야 한다.

### 2.2.3 kafka-console-consumer.sh

- 토픽으로 전송한 데이터는 kafka-console-consumer 명령어로 확인할 수 있고 --from-beginning 옵션을 주면 가장 처음 데이터부터 출력한다.
  - `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning`
- 만약 데이터의 키와 값을 확인하고 싶다면 --property 옵션을 사용한다.
  - `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --property print.key=true --property key.separator="-" --group test-group --from-beginning`
  - 위 명령어에서 group 옵션을 통해 컨슈머 그룹을 생성했다. 컨슈머 그룹은 1개 이상의 컨슈머로 이루어져 있다. 컨슈머 그룹을 통해 가져간 토픽의 메시지는 가져간 메시지에 대해 커밋을 한다. 커밋이란 컨슈머가 특정 레코드까지 처리를 완료했다고 레코드의 오프셋 번호를 브로커에 저장하는 것이다.
- kafka-console-consumer 명령어로 데이터를 가져가게 되면 토픽의 모든 파티션으로부터 동일한 중요도로 데이터를 가져간다. 이로 인해 프로슈서가 넣은 데이터의 순서와 컨슈머가 가져가는 데이터의 순서가 달라질 수 있다. 만약 토픽에 넣은 데이터의 순서를 보장하고 싶다면 가장 좋은 방법은 파티션 1개로 구성된 토픽을 만드는 것이다. 한 개의 파티션에서는 데이터의 순서를 보장하기 때문이다.

### 2.2.4 kafka-consumer-group.sh

- 컨슈머 그룹은 따로 생성 명령어를 날리지 않고 컨슈머가 동작할 때 그룹이름을 지정하면 새로 생성된다. 생성된 컨슈머 그룹의 리스트는 아래 명령어로 확인 가능하다.
  - `bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list`
- 컨슈머 그룹의 이름을 토대로 어떤 토픽의 데이터를 가져가는지 확인하려면 아래 명령어가 사용된다.
  - `bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --describe`
  - 컨슈머 그룹의 상세정보를 확인하는 것은 컨슈머 개발이나 카프카 운영할 때 중요하게 활용된다. 컨슈머 그룹이 중복되지 않았는지 컨슈머 랙이 있진 않은지 활용할 수 있다.

### 2.2.5 kafka-verifiable-producer, consumer.sh

- string 메시지를 주고 받으며, 카프카 클러스터 설치가 완료되고 간단한 네트워크 통신 테스트를 할 때 유용하다.
  - `bin/kafka-verifiable-producer.sh --bootstrap-server localhost:9092 --max-message 5 --topic verify-test`
  - `bin/kafka-verifiable-consumer.sh --bootstrap-server localhost:9092 --topic verify-test --group-id test-group`

### 2.2.6 kafka-delete-record.sh

- 이미 적재된 토픽의 데이터를 지우는 방법으로 kafka-delete-record.sh을 사용할 수 있다. 이미 적재된 데이터 중 가장 오래된 데이터부터 특정 시점의 오프셋까지 삭제할 수 있다. 예를 들어 0번 부터 10번까지 데이터를 지우고 싶다면 아래와 같이 입력한다.
  - delete.json 파일을 작성
    - `{"partitions": [{"topic":"test", "partition":0, "offset":3}], "version":1}]`
  - `bin/kafka-delete-records.sh --bootstrap-server localhost:9092 --offset-json-file delete.json`

## 3. 카프카 기본 개념 설명

### 3.1 카프카 브로커 / 클러스터 / 주키퍼

- 브로커는 데이터를 주고받기 위해 사용하는 주체이자, 데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션이다. 데이터를 안전하게 보관하고 처리하기 위해 3대 이상의 브로커 서버를 1개의 클러스터로 묶어 운영한다.
- 프로듀서가 보낸 데이터를 브로커가 토픽의 파티션에 데이터를 저장하고, 컨슈머 요청이 들어오면 데이터를 전달한다. 프로듀서가 전달한 데이터는 파일 시스템에 저장된다.
- 파일 시스템에 저장되기 때문에 파일 I/O로 속도가 느리지 않을까 걱정할 수 있다. 카프카는 페이지 캐시를 사용하여 디스크 I/O 속도를 높여 이 문제를 해결한다. 페이지 캐시란 OS에서 파일 입출력 성능 향상을 위해 만들어 놓은 메모리 영역으로 한 번 읽은 파일의 내용을 메모리의 페이지 캐시 영역에 저장하고, 추후 동일한 파일 접근이 일어나면 메모리에서 직접 읽는 방식이다.
- 데이터 복제는 카프카를 장애허용 시스템으로 동작하게 만든다. 데이터 복제는 파티션 단위로 이루어지며 토픽을 생성할 때 파티션 복제 개수(replication factor)를 설정할 수 있고 기본 값은 브로커에 설정된 옵션을 따라간다.
- 프로듀서 / 컨슈머와 직접 통신하는 파티션을 리더 파티션이라고 부르고 나머지 복제본은 팔로워 파티션이라 부른다. 팔로워 파티션들은 리더 파티션의 오프셋을 확인해 자신의 오프셋과 차이가 나는 경우 리더 파티션의 데이터를 가져와 자신의 파티션에 저장하는데 이를 복제라고 부른다.
- 3개의 브로커로 이루어진 클러스트에서 리더 파티션에 장애가 발생해도 팔로워 파티션 중 하나가 리더 지위를 넘겨받아 데이터 유실 없이 프로듀서 / 컨슈머와 동작한다. 데이터가 일부 유실되어도 무관하고 속도가 중요하다면 복제 개수를 1 or 2로 설정한다. 금융 정보와 같이 유실되어선 안되는 데이터의 경우 복제 개수를 3으로 설정해 데이터를 안정적으로 유지해야 한다.
- 다수 브로커 중 한대가 컨트롤러의 역할을 한다. 컨트롤러는 다른 브로커들의 상태를 체크하고 브로커에 장애가 나거나 빠지는 경우 해당 브로커에 존재하는 리더 파티션을 재분배한다. 만약 컨트롤러 역할을 하는 브로커에 장애가 발생하면 다른 브로커가 컨트롤러 역할을 한다.
- 주키퍼는 카프카의 메타 데이터(host, port, 어떤 브로커가 컨트롤러인지, 저장된 토픽)를 관리하는데 사용된다.

### 3.2 토픽과 파티션

- 토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위다. 토픽은 1개 이상의 파티션을 소유하며 파티션에 저장된 데이터를 레코드라 부른다.
- 컨슈머 처리량이 한정된 상황에서 처리량을 증가하려면 가장 좋은 방법은 컨슈머의 개수를 늘림과 동시에 파티션 개수도 늘리는 것이다.
- 파티션은 큐와 비슷한 구조로 먼저 들어간 레코드는 컨슈머가 먼저 가져가게 되며, 데이터를 가져가면 카프카는 데이터를 삭제하지 않는다.
- 토픽 이름 조건
  - 토픽 이름은 케밥 케이스 (-)나 스네이크 케이스(_)를 소문자로 사용하는게 좋으며 이름 예시를 들면 아래와 같다.
  - [환경].[팀명].[애플리케이션명].[메시지타입]
    - prod.marketing-team.sms-platform.json
  - [프로젝트명].[서비스명].[환경].[이벤트명]
    - community.payment.prod.notification
  - [환경].[서비스명].[JIRA번호].[메시지타입]
    - dev.eamil-sender.jira-1234.email-vo-custom
  - 중요한 것은 토픽이름에 대한 규칙을 정하고 따르는 것으로, 카프카는 토픽이름 변경을 지원하지 않으므로 삭제 후 다시 생성하는 것 밖에 방법이 없다.

### 3.3 레코드

- 프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장된다. 브로커에 한 번 적재된 레코드는 수정될 수 없고 리텐션 기간 or 용량에 따라서만 삭제된다.

### 3.4 카프카 클라이언트

- 카프카 클라이언트 라이브러리는 카프카 클러스터에 명령을 내리거나 데이터를 송/수신하기 위해 사용하며 카프카 클라이언트는 프레임워크나 애플리케이션 위에서 구현하고 실행해야 한다.

#### 3.4.1 프로듀서 API

- 코드단에서 kafkaProducer 인스턴스가 send 메서드를 호출하면 파티셔너에서 토픽의 어떤 파티션으로 전송될 것인지 정해진다. 파티셔너에 의해 구분된 레코드는 데이터를 전송하기 전에 버퍼에 쌓아놓고 전송함으로써 프로듀서의 처리량 향상에 도움을 준다.
- 카프카 클라이언트 라이브러리 2.5.0 버전에서 파티셔너를 지정하지 않으면 UniformStickyPartitioner 가 기본 설정된다.
RoundRobinPartitioner 와의 차이는 배치에 묶이는 데이터의 양이다. UniformStickyPartitioner는 어큐뮬레이터에서 데이터가 배치로 모두 묶일때까지 기다렸다가 배치에 묶인 데이터를 동일한 파티션에 전송해 향상된 성능을 가지게 되었다.
- 프로듀서가 데이터를 보낼 때 응답을 동기로 기다릴지 비동기로 받을지 결정할 수 있는데, 만약 데이터 순서가 중요하다면 동기로 전송 결과를 응답받아야 한다. 비동기로 보낼 경우, 비동기로 A 다음 B를 보냈다고 가정하자. A가 실패하고 B가 성공하면 데이터 순서가 바뀔 수 있다.

#### 3.4.2 컨슈머 API

- 토픽의 파티션으로부터 데이터를 가져가기 위해 컨슈머를 운영하는 방법은 크게 2가지가 있다.
  - 1개 이상의 컨슈머로 이루어진 컨슈머 그룹을 운영하거나
  - 토픽의 특정 파티션만 구독하는 컨슈머를 운영하는 것이다.
- 컨슈머 그룹으로 운영한다면 파티션과 컨슈머의 관계를 1대 다로 봐도 된다. 한 파티션은 하나의 컨슈머랑 매핑되며 반대로 하나의 컨슈머를 여러 파티션에 할당될 수 있다. 이런 특징으로 컨슈머 그룹의 컨슈머 개수는 토픽의 파티션 개수와 같거나 작아야 한다.
- 만약 3개의 파티션을 가진 토픽이 있다면 3개 이하의 컨슈머로 구성된 컨슈머 그룹으로 운영해야 한다. 만약 4개의 컨슈머가 있다면 1개의 컨슈머를 놀게된다.
- 컨슈머 그룹으로 운영할 때의 장점은 다른 컨슈머 그룹과 격리되어 영향을 받지 않는다. 예를 들어 리소스 정보가 토픽에 저장된다면 이를 소비하는 ES 컨슈머 그룹과 하둡 컨슈머 그룹이 있을 때 ES에 장애가 발생해도 하둡 컨슈머 그룹은 영향을 받지 않는다.
- 컨슈머 그룹의 컨슈머에 장애가 발생하면, 장애가 발생한 컨슈머에 할당된 파티션의 소유권이 정상 컨슈머에게 넘어간다. 이 과정을 리밸런싱이라고 부르며 컨슈머가 추가되거나 제외될 때 일어난다.
- 컨슈머는 브로커로부터 데이터를 어디까지 가져갔는지 커밋을 통해 기록한다. 특정 토픽의 파티션을 어떤 컨슈머 그룹이 몇 번째까지 가져갔는지 브로커 내부의 내부 토픽(_consumer_offsets)에 기록된다. 만약 컨슈머 동작 문제로 오프셋 커밋이 실패했다면 데이터 처리의 중복이 발생할 수 있다.
- 오프셋 커밋은 명시적, 비명시적으로 수행할 수 있고, 기본옵션은 poll 메소드가 수행될 때 일정 간격마다 오프셋을 커밋하도록 설정되어 있다. 편리한 방식이지만 poll 메서드 호출 이후에 리밸런싱 or 컨슈머 강제 종료시 데이터 중복 or 유실의 문제가 있다. 그러므로 데이터 중복이나 유실을 허용하지 않는다면 명시적으로 오프셋을 커밋해야 한다.
- 명시적으로 오프셋을 커밋하려면 poll 메서드 호출된 후 반환받은 데이터의 처리가 완료되고 commitSync을 호출하면 된다. commitSync은 브로커에 커밋 요청을 하고 응답을 기다리므로 컨슈머 처리량에 영향을 미치므로 운영 목적에 맞게 설정해야 한다.
- 컨슈머 그룹에서 컨슈머가 추가/삭제되면 파티션을 컨슈머에 재할당하는 리밸런스가 일어난다. poll 메서드를 통해 반환받은 데이터를 모두 처리하기 전에 리밸런스가 발생하면 데이터를 중복처리할 수 있다. Poll 메서드를 통해 받은 데이터 일부를 처리했으나 커밋하지 않았기 때문이다. 리밸런스 발생 시 데이터를 중복처리하지 않기 위해서는 처리한 데이터를 기준으로 커밋을 시도해야 한다.

#### 3.4.3 어드민 API

- 실제 운영환경에서는 카프카에 설정된 내부 옵션을 설정하고 확인하는 것이 중요하다. 내부 옵션을 확인하는 확실한 방법은 브로커 중 한대에 접속해 브로커 옵션을 확인하는 것이지만 매우 번거로운 작업이다.
- 카프카 클라이언트에서는 내부 옵션을 설정하거나 조회하기 위해 AdminClient 클래스를 제공한다. AdminClient를 활용해 ACL 접근 규칙을 추가하거나 특정 토픽의 데이터 양이 늘어남을 감지하고 토픽의 파티션을 늘리는 등의 작업을 할 수 있다.
